
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>37. Linear models &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/ModelingOptimization/LinearModels';</script>
    <script src="../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="38. Mathematical optimization" href="MathematicalOptimization.html" />
    <link rel="prev" title="36. Overview of modeling" href="OverviewModeling.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Intro/Overview.html">1. Invitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/Introduction.html">2. Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/MoreBayesTheorem.html">4.7. More on Bayes’ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Exploring_pdfs.html">5.1. Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/visualization_of_CLT.html">Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/chi_squared_tests.html">5.4. Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When don’t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping:</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/demo-BayesianBasics.html">6.6. Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Bayesian_updating_coinflip_interactive.html">6.7. Widgetized coin tossing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/dealing_with_outliers.html">9.5. Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/Multimodel_inference.html">12. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/ModelSelection/ModelSelection.html">12.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/MD_balldrop_v1.html">13.4. Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../StochasticProcesses/RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../StochasticProcesses/StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../StochasticProcesses/MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/MCMC.html">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../StochasticProcesses/Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../StochasticProcesses/Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../StochasticProcesses/BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/CF/demo-GaussianProcesses.html">demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/LogReg/LogReg.html">21. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/MachineLearningExamples.html">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/ANN/MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">22.7. ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/random_initialized_ANN_vs_width.html">23.3. Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/Student_t_distribution_from_Gaussians.html">28. Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/linear_algebra_games_including_SVD.html">29.5. SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1"><a class="reference internal" href="OverviewModeling.html">36. Overview of modeling</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">37. Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="MathematicalOptimization.html">38. Mathematical optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Setup/exercise_Intro_01_Jupyter_Python.html">40. Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/exercise_Intro_03_Numpy.html">41.5. Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/demo-Intro.html">41.6. Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/Simple_widgets_v1.html">41.7. Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_I_toy_model_of_EFT.html">MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/model-selection_mini-project-IIa.html">MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_IIIa_bayesian_optimization.html">MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/ModelingOptimization/LinearModels.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/LearningFromData-content/ModelingOptimization/LinearModels.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-linear-models">37.1. Definition of linear models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-with-linear-models">37.2. Regression analysis with linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-equation">The normal equation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-linear-regression-warmup">37.3. Ordinary linear regression: warmup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-linear-regression-in-practice">37.4. Ordinary linear regression in practice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">37.5. Solutions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- !split -->
<section class="tex2jax_ignore mathjax_ignore" id="linear-models">
<span id="sec-linearmodels"></span><h1><span class="section-number">37. </span>Linear models<a class="headerlink" href="#linear-models" title="Link to this heading">#</a></h1>
<blockquote class="epigraph">
<div><blockquote>
<div><p>“As always in life, people want a simple answer; … and it’s always wrong.”</p>
</div></blockquote>
<p class="attribution">—Susan Greenfield</p>
</div></blockquote>
<p>In this chapter we will discuss models <span class="math notranslate nohighlight">\(\modeloutput\)</span> that are linear in their parameters <span class="math notranslate nohighlight">\(\pars\)</span>. Although linear models are simple, they are sometimes useful for analyzing real-world data.</p>
<section id="definition-of-linear-models">
<h2><span class="section-number">37.1. </span>Definition of linear models<a class="headerlink" href="#definition-of-linear-models" title="Link to this heading">#</a></h2>
<p>In <strong>linear modeling</strong> the dependence on the model parameters <span class="math notranslate nohighlight">\(\pars\)</span> is <strong>linear</strong>, and this fact will make it possible to express that regression analysis as a linear algebra problem, and as we will show it will be possible to find an analytical expression for the optimal set of model parameters. Note that we will mostly operate with models depending on more than one parameter. Hence, we denote the parameters (<span class="math notranslate nohighlight">\(\pars\)</span>) using a bold symbol. In this chapter we will, however, consider models (<span class="math notranslate nohighlight">\(\modeloutput\)</span>) that relate a single dependent variable (<span class="math notranslate nohighlight">\(\output\)</span>) with a single independent one (<span class="math notranslate nohighlight">\(\inputt\)</span>).</p>
<p>The linear parameter dependence implies that our model separates into a sum of parameters times basis functions. Assuming <span class="math notranslate nohighlight">\(N_p\)</span> different basis functions we have</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-model">
<span class="eqno">(37.1)<a class="headerlink" href="#equation-eq-linear-model" title="Link to this equation">#</a></span>\[
\model{\pars}{\inputt} = \sum_{j=0}^{N_p-1} \para_j f_j(\inputt).
\]</div>
<p>Note that there is no <span class="math notranslate nohighlight">\(\pars\)</span>-dependence in the basis functions <span class="math notranslate nohighlight">\(f_j(\inputt)\)</span>.</p>
<p>From a machine-learning perspective the different basis functions are known as <strong>features</strong>.</p>
<div class="proof example admonition" id="example:polynomial-linear-model">
<p class="admonition-title"><span class="caption-number">Example 37.1 </span> (Polynomial basis functions)</p>
<section class="example-content" id="proof-content">
<p>A common linear model corresponds to the use of polynomial basis functions <span class="math notranslate nohighlight">\(f_j(x) = x^j\)</span>. A polynomial model of degree <span class="math notranslate nohighlight">\(N_p-1\)</span> would then be written</p>
<div class="math notranslate nohighlight" id="equation-eq-polynomial-basis">
<span class="eqno">(37.2)<a class="headerlink" href="#equation-eq-polynomial-basis" title="Link to this equation">#</a></span>\[
M(\pars;\inputt) = \sum_{j=0}^{N_p-1} \para_j \inputt^j.
\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(j=0\)</span> basis function is <span class="math notranslate nohighlight">\(f_0(x) = x^0 = 1\)</span> such that the <span class="math notranslate nohighlight">\(\para_0\)</span> parameter becomes the intercept.</p>
</section>
</div><div class="proof example admonition" id="example:LinearModels:liquid-drop-model">
<p class="admonition-title"><span class="caption-number">Example 37.2 </span> (Liquid-drop model for nuclear binding energies)</p>
<section class="example-content" id="proof-content">
<p>The liquid drop model is useful for a phenomenological description of nuclear binding energies (BE) as a function of the mass number <span class="math notranslate nohighlight">\(A\)</span> and the number of protons <span class="math notranslate nohighlight">\(Z\)</span>, neutrons <span class="math notranslate nohighlight">\(N\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-fee63aa1-2b80-47b5-9619-8ffcf224ea8b">
<span class="eqno">(37.3)<a class="headerlink" href="#equation-fee63aa1-2b80-47b5-9619-8ffcf224ea8b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{BE}(A,N,Z) = a_0+a_1A+a_2A^{2/3}+a_3 Z^2 A^{-1/3}+a_4 (N-Z)^2 A^{-1}.
\end{equation}\]</div>
<p>We have five features: the intercept (constant term, bias), the <span class="math notranslate nohighlight">\(A\)</span> dependent volume term, the <span class="math notranslate nohighlight">\(A^{2/3}\)</span> surface term and the Coulomb <span class="math notranslate nohighlight">\(Z^2 A^{-1/3}\)</span> and pairing <span class="math notranslate nohighlight">\((N-Z)^2 A^{-1}\)</span> terms. Although the features are somewhat complicated functions of the independent variables <span class="math notranslate nohighlight">\(A,N,Z\)</span>, we note that the <span class="math notranslate nohighlight">\(p=5\)</span> regression parameters <span class="math notranslate nohighlight">\(\pars = (a_0, a_1, a_2, a_3, a_4)\)</span> enter linearly.</p>
</section>
</div></section>
<section id="regression-analysis-with-linear-models">
<h2><span class="section-number">37.2. </span>Regression analysis with linear models<a class="headerlink" href="#regression-analysis-with-linear-models" title="Link to this heading">#</a></h2>
<p>When performing a regression analysis with a linear model, i.e., doing linear regression, we have access to a set of data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> for the dependent variable, i.e.,</p>
<div class="amsmath math notranslate nohighlight" id="equation-474b423f-a433-44dc-aa1e-a19e0ba5d154">
<span class="eqno">(37.4)<a class="headerlink" href="#equation-474b423f-a433-44dc-aa1e-a19e0ba5d154" title="Permalink to this equation">#</a></span>\[\begin{equation}
\data = [y_1, y_2,\dots, y_{N_d}]^T.
\end{equation}\]</div>
<p>For each datum there is an independent variable <span class="math notranslate nohighlight">\(x_i\)</span>, and our model for each datum <span class="math notranslate nohighlight">\(y_i\)</span> is</p>
<div class="amsmath math notranslate nohighlight" id="equation-5e57c99d-069f-41db-9b01-e6fd60e2e581">
<span class="eqno">(37.5)<a class="headerlink" href="#equation-5e57c99d-069f-41db-9b01-e6fd60e2e581" title="Permalink to this equation">#</a></span>\[\begin{equation}
M_i \equiv M(\pars;x_i) = \sum_{j=0}^{N_p-1} \para_j f_j(x_i).
\end{equation}\]</div>
<p>We can collect the basis function evaluated at each independent variable <span class="math notranslate nohighlight">\(x_i\)</span> in a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> of dimension <span class="math notranslate nohighlight">\(N_d \times N_p\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-linearmodels-design-matrix">
<span class="eqno">(37.6)<a class="headerlink" href="#equation-eq-linearmodels-design-matrix" title="Link to this equation">#</a></span>\[\begin{split}
\dmat = 
	\begin{bmatrix} 
        f_0(x_1) &amp; \ldots &amp; f_{N_p-1}(x_1) \\
        f_0(x_2) &amp; \ldots &amp; f_{N_p-1}(x_2) \\
        \vdots  &amp; \ddots &amp; \vdots \\
        f_0(x_{N_d}) &amp; \ldots &amp; f_{N_p-1}(x_{N_d})
    \end{bmatrix}
\end{split}\]</div>
<p>This matrix will be referred to as a <strong>design matrix</strong>.</p>
<div class="proof example admonition" id="example:design-matrix-polynomial-models">
<p class="admonition-title"><span class="caption-number">Example 37.3 </span> (The design matrix for polynomial models)</p>
<section class="example-content" id="proof-content">
<p>The design matrix for a linear model with polynomial basis functions becomes</p>
<div class="amsmath math notranslate nohighlight" id="equation-2f6ffc80-97b5-4a6a-b3c6-153858c34086">
<span class="eqno">(37.7)<a class="headerlink" href="#equation-2f6ffc80-97b5-4a6a-b3c6-153858c34086" title="Permalink to this equation">#</a></span>\[\begin{equation}
\dmat =
\begin{bmatrix} 
1&amp; x_{1}^1 &amp;x_{1}^2&amp; \dots &amp; \dots &amp;x_{1}^{p-1}\\
1&amp; x_{2}^1 &amp;x_{2}^2&amp; \dots &amp; \dots &amp;x_{2}^{p-1}\\
1&amp; x_{3}^1 &amp;x_{3}^2&amp; \dots &amp; \dots &amp;x_{3}^{p-1}\\                      
\dots&amp; \dots &amp;\dots&amp; \dots &amp; \dots &amp;\dots\\
1&amp; x_{N_d}^1 &amp;x_{N_d}^2&amp; \dots &amp; \dots &amp;x_{N_d}^{p-1}\\
\end{bmatrix}, 
\end{equation}\]</div>
<p>where we are considering a polynomial of degree <span class="math notranslate nohighlight">\(p-1\)</span> which implies a model with <span class="math notranslate nohighlight">\(p\)</span> features (including the intercept). It is also known within linear algebra as a <a class="reference external" href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde matrix</a>.</p>
</section>
</div><p>Next, we introduce a column vector for the parameters</p>
<div class="amsmath math notranslate nohighlight" id="equation-4bc41eb0-5501-4c20-a29e-1d69c1d57b6b">
<span class="eqno">(37.8)<a class="headerlink" href="#equation-4bc41eb0-5501-4c20-a29e-1d69c1d57b6b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pars = [\para_0,\para_1, \para_2,\dots, \para_{N_p-1}]^T,
\end{equation}\]</div>
<p>and we arrive at the matrix equation</p>
<div class="amsmath math notranslate nohighlight" id="equation-c45974cb-bcb9-44df-8d2b-4c2857109903">
<span class="eqno">(37.9)<a class="headerlink" href="#equation-c45974cb-bcb9-44df-8d2b-4c2857109903" title="Permalink to this equation">#</a></span>\[\begin{equation}
\data = \dmat \pars+\boldsymbol{\epsilon}.
\end{equation}\]</div>
<p>The last term <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is a column vector of so-called <strong>residuals</strong>. This term expresses the part of the dependent variable, for which we have data, that we cannot describe using a linear model. Formally, we can therefore write <span class="math notranslate nohighlight">\(\epsilon_i = y_i - M_i\)</span> and define the vector as</p>
<div class="amsmath math notranslate nohighlight" id="equation-7322a705-eda8-44ab-bc0a-f79e40f28e86">
<span class="eqno">(37.10)<a class="headerlink" href="#equation-7322a705-eda8-44ab-bc0a-f79e40f28e86" title="Permalink to this equation">#</a></span>\[\begin{equation}
\residuals = [\residual_1,\residual_2, \residual_3,\dots, \residual_{N_d}]^T.
\end{equation}\]</div>
<p>It is important to realize that our model <span class="math notranslate nohighlight">\(M\)</span> provides an approximate description of the data. Indeed, <em>all models are wrong</em> and in a realistic setting we have no guarantee that the data is generated by a linear process. Of course, based on physics insight, or other assumptions, there might exists very good reasons for using a linear model to explain the data.</p>
<section id="the-normal-equation">
<h3>The normal equation<a class="headerlink" href="#the-normal-equation" title="Link to this heading">#</a></h3>
<p>A regression analysis often aims at finding the model parameters <span class="math notranslate nohighlight">\(\pars\)</span> of a model <span class="math notranslate nohighlight">\(M\)</span> such that the vector of errors <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is minimized in the sense of its Euclidean norm (or 2-norm). You might ask the very relevant question why this particular goal is desirable. We will return to this consideration in <a class="reference internal" href="../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html#sec-bayesianlinearregression"><span class="std std-ref">Bayesian Linear Regression (BLR)</span></a>. Nevertheless, in order to find the “optimal” set of parameters <span class="math notranslate nohighlight">\(\pars^*\)</span> we seek to minimize</p>
<div class="math notranslate nohighlight" id="equation-eq-linearregression-cost-function">
<span class="eqno">(37.11)<a class="headerlink" href="#equation-eq-linearregression-cost-function" title="Link to this equation">#</a></span>\[
C(\pars)\equiv \sum_{i=1}^{N_d} \epsilon_i^2 = \sum_{i=1}^{N_d}\left(y_i-M_i\right)^2 = \left\{\left(\data-\dmat \pars\right)^T\left(\data-\dmat \pars\right)\right\}.
\]</div>
<p>The solution to this optimization problem turns out to be a solution of the normal equation and is known as ordinary least-squares or ordinary linear regression.</p>
<div class="proof theorem admonition" id="theorem:LinearModels:normal-equation">
<p class="admonition-title"><span class="caption-number">Theorem 37.1 </span> (Ordinary least squares (the normal equation))</p>
<section class="theorem-content" id="proof-content">
<p>The ordinary least-squares method corresponds to finding the optimal parameter vector <span class="math notranslate nohighlight">\(\pars^*\)</span> that minimizes the Euclidean norm of the residual vector <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} = \data - \dmat \pars\)</span>, where <span class="math notranslate nohighlight">\(\data\)</span> is a column vector of observations and <span class="math notranslate nohighlight">\(\dmat\)</span> is the design matrix <a class="reference internal" href="#equation-eq-linearmodels-design-matrix">(37.6)</a>.</p>
<p>Finding this optimum turns out to correspond to solving the <strong>normal equation</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-normalequation">
<span class="eqno">(37.12)<a class="headerlink" href="#equation-eq-normalequation" title="Link to this equation">#</a></span>\[
\dmat^T\data = \dmat^T\dmat\pars^*.  
\]</div>
<p>Given that the <strong>normal matrix</strong> <span class="math notranslate nohighlight">\(\dmat^T\dmat\)</span> is invertible, the solution to the normal equation is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-linearmodels-ols-optimum">
<span class="eqno">(37.13)<a class="headerlink" href="#equation-eq-linearmodels-ols-optimum" title="Link to this equation">#</a></span>\[
\pars^* =\left(\dmat^T\dmat\right)^{-1}\dmat^T\data.
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Due to its quadratic form, the Euclidean norm <span class="math notranslate nohighlight">\(\left| \boldsymbol{\epsilon} \right|_2^2 = \left(\data-\dmat\pars\right)^T\left(\data-\dmat\pars\right) \equiv C(\pars)\)</span> is bounded from below and we just need to find the single extremum. That is we need to solve the problem</p>
<div class="amsmath math notranslate nohighlight" id="equation-8b7b7a59-b457-44ce-8b0f-652d394a3920">
<span class="eqno">(37.14)<a class="headerlink" href="#equation-8b7b7a59-b457-44ce-8b0f-652d394a3920" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pars^* =
{\displaystyle \mathop{\mathrm{arg} \min}_{\pars\in
{\mathbb{R}}^{N_p}}} \left(\data-\dmat\pars\right)^T\left(\data-\dmat\pars\right).
\end{equation}\]</div>
<p>In practical terms it means we will require</p>
<div class="amsmath math notranslate nohighlight" id="equation-cdefcfb9-910c-4669-96aa-0dd1eb768b9d">
<span class="eqno">(37.15)<a class="headerlink" href="#equation-cdefcfb9-910c-4669-96aa-0dd1eb768b9d" title="Permalink to this equation">#</a></span>\[\begin{align}
\frac{\partial C(\pars)}{\partial \para_j} = \frac{\partial }{\partial \para_j} \Bigg[  \sum_{i=1}^{N_d}\Big(y_i &amp;-\para_0 f_0(x_i)-\para_1f_1(x_i)-\para_2f_2(x_i)-\dots \\
&amp;-  \para_{N_p-1}f_{N_p-1}(x_i)\Big)^2\Bigg] = 0, 
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(f_j(x_i)\)</span> are the elements of <span class="math notranslate nohighlight">\(\data\)</span> and <span class="math notranslate nohighlight">\(\dmat\)</span>, respectively. Performing the derivative results in</p>
<div class="math notranslate nohighlight" id="equation-eq-linearmodels-gradient-elements">
<span class="eqno">(37.16)<a class="headerlink" href="#equation-eq-linearmodels-gradient-elements" title="Link to this equation">#</a></span>\[\begin{split}
\frac{\partial C(\pars)}{\partial \para_j} = -2\Bigg[ \sum_{i=1}^{N_d}f_j(x_i)\Big(y_i &amp;-\para_0 f_0(x_i)-\para_1f_1(x_i)-\para_2f_2(x_i)-\dots \\
&amp;-\para_{N_p-1}f_{N_p-1}(x_i)\Big)\Bigg]=0,
\end{split}\]</div>
<p>which is one element of the full gradient vecor. This gradient vector can be succinctly expressed in matrix-vector form as</p>
<div class="math notranslate nohighlight" id="equation-eq-linearregression-gradient">
<span class="eqno">(37.17)<a class="headerlink" href="#equation-eq-linearregression-gradient" title="Link to this equation">#</a></span>\[
\boldsymbol{\nabla}_{\pars} C(\pars) = -2 \dmat^T\left( \data-\dmat\pars\right).  
\]</div>
<p>The minimum of <span class="math notranslate nohighlight">\(C\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\nabla}_{\pars} C(\pars) = 0\)</span>, then corresponds to</p>
<div class="math notranslate nohighlight">
\[
\dmat^T\data = \dmat^T\dmat\pars^*,  
\]</div>
<p>which is the normal equation. Finally, if the matrix <span class="math notranslate nohighlight">\(\dmat^T\dmat\)</span> is invertible then we have the solution</p>
<div class="math notranslate nohighlight">
\[
\pars^* =\left(\dmat^T\dmat\right)^{-1}\dmat^T\data.
\]</div>
</div>
<p>We note also that since our design matrix is defined as <span class="math notranslate nohighlight">\(\dmat\in
{\mathbb{R}}^{N_d\times N_p}\)</span>, the product <span class="math notranslate nohighlight">\(\dmat^T\dmat \in
{\mathbb{R}}^{N_p\times N_p}\)</span>. The product <span class="math notranslate nohighlight">\(\left(\dmat^T\dmat\right)^{-1}\dmat^T\)</span> is called the pseudo-inverse of the design matrix <span class="math notranslate nohighlight">\(\dmat\)</span>. The pseudo-inverse is a generalization of the usual matrix inverse. The former can be defined for also for non-square matrices that are not necessarily full rank. In the case of full-rank and square matrices the pseudo-inverse is equal to the usual inverse.</p>
<p>The regression residuals <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}^{*} =  \data - \dmat \pars^{*}\)</span> can be used to obtain an estimator <span class="math notranslate nohighlight">\(s^2\)</span> of the variance of the residuals</p>
<div class="math notranslate nohighlight">
\[
s^2 = \frac{(\boldsymbol{\epsilon}^*)^T\boldsymbol{\epsilon}^*}{N_d-N_p},
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_p\)</span> is the number of parameters in the model and <span class="math notranslate nohighlight">\(N_d\)</span> is the number of data.</p>
</section>
</section>
<section id="ordinary-linear-regression-warmup">
<span id="sec-ols-warmup"></span><h2><span class="section-number">37.3. </span>Ordinary linear regression: warmup<a class="headerlink" href="#ordinary-linear-regression-warmup" title="Link to this heading">#</a></h2>
<p>To warm up, and get acquainted with the notation and formalism, let us work out a small example. Assume that we have the situation where we have collected two datapoints <span class="math notranslate nohighlight">\(\data = [y_1,y_2]^T = [-3,3]^T\)</span> for the predictor values <span class="math notranslate nohighlight">\([x_1,x_2]^T = [-2,1]^T\)</span>.</p>
<p>This data could have come from any process, even a non-linear one. But this is artificial data that I generated by evaluating the function <span class="math notranslate nohighlight">\(y = 1 + 2x\)</span> at <span class="math notranslate nohighlight">\(x=x_1=-2\)</span> and <span class="math notranslate nohighlight">\(x=x_2=1\)</span>. Clearly, the data-generating mechanism is very simple and corresponds to a linear model <span class="math notranslate nohighlight">\(y = \theta_0 + \theta_1 x\)</span> with <span class="math notranslate nohighlight">\([\theta_0,\theta_1] = [1,2]\)</span>. This is the kind of information we <em>never</em> have in reality. Indeed, we are always uncertain about the process that maps input to output, and as such our model <span class="math notranslate nohighlight">\(M\)</span> will always be wrong. We are also uncertain about the parameters <span class="math notranslate nohighlight">\(\pars\)</span> of our model. These are the some of the fundamental reasons for why it can be useful to operate with a Bayesian approach where we can assign probabilities to any quantity and statement. In this example, however, we will continue with the standard (frequentist) approach based on finding the parameters that minimize the squared errors (i.e., the norm of the residual vector).</p>
<p>We will now assume a linear model with polynomial basis up to order one to model the data, i.e.,</p>
<div class="math notranslate nohighlight">
\[
M(\pars;\inputt) = \para_0 + \para_1 \inputt,
\]</div>
<p>which we can express in terms of a design matrix <span class="math notranslate nohighlight">\(\dmat\)</span> and (unknown) parameter vector <span class="math notranslate nohighlight">\(\pars\)</span> as <span class="math notranslate nohighlight">\(M = \dmat \pars\)</span>.</p>
<p>In the present case the two unknowns <span class="math notranslate nohighlight">\(\pars = [\para_0,\para_1]^T\)</span> can be fit to the two datapoints <span class="math notranslate nohighlight">\(\data = [-3,3]^T\)</span> using pen a paper.</p>
<div class="exercise admonition" id="exercise:ols_example_1">

<p class="admonition-title"><span class="caption-number">Exercise 37.1 </span></p>
<section id="exercise-content">
<p>In the example above you have two data points and two unknowns, which means you can easily solve for the model parameters using a conventional matrix inverse.
Do the numerical calculation to make sure you have setup the problem correctly.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:ols_example_2">

<p class="admonition-title"><span class="caption-number">Exercise 37.2 </span></p>
<section id="exercise-content">
<p>Evaluate the normal equations for the design matrix <span class="math notranslate nohighlight">\(\dmat\)</span> and data vector <span class="math notranslate nohighlight">\(\data\)</span> in the example above.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:ols_example_3">

<p class="admonition-title"><span class="caption-number">Exercise 37.3 </span></p>
<section id="exercise-content">
<p>Evaluate the sample variance <span class="math notranslate nohighlight">\(s^2\)</span> for the example above. Do you think the result makes sense?</p>
</section>
</div>
</section>
<section id="ordinary-linear-regression-in-practice">
<span id="sec-ols-in-practice"></span><h2><span class="section-number">37.4. </span>Ordinary linear regression in practice<a class="headerlink" href="#ordinary-linear-regression-in-practice" title="Link to this heading">#</a></h2>
<p>We often have situation where we have much more than just two datapoints, and they rarely  fall exactly on a straight line. Let’s use python to generate some more realistic, yet artificial, data. Using the function below you can generate data from some linear process with random variables for the underlying parameters. We call this a data-generating process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">data_generating_process_reality</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;polynomial&#39;</span><span class="p">:</span>
      <span class="n">true_params</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;poldeg&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,))</span>
      <span class="c1">#polynomial model   </span>
      <span class="k">def</span><span class="w"> </span><span class="nf">process</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">xdata</span><span class="p">):</span>
          <span class="n">ydata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polynomial</span><span class="o">.</span><span class="n">polynomial</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span><span class="n">params</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">ydata</span>
      
  <span class="c1"># use this to define a non-polynomial (possibly non-linear) data-generating process</span>
  <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s1">&#39;nonlinear&#39;</span><span class="p">:</span>
      <span class="n">true_params</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">def</span><span class="w"> </span><span class="nf">process</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">xdata</span><span class="p">):</span>
          <span class="n">ydata</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">xdata</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
          <span class="k">return</span> <span class="n">ydata</span>           
  <span class="k">else</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Unknown Model&#39;</span><span class="p">)</span>
      
  <span class="c1"># return function for the true process the values for the true parameters</span>
  <span class="c1"># and the name of the model_type</span>
  <span class="k">return</span> <span class="n">process</span><span class="p">,</span> <span class="n">true_params</span><span class="p">,</span> <span class="n">model_type</span>    
</pre></div>
</div>
</div>
</div>
<p>Next, we make some measurements of this process, and that typically entails some measurement errors. We will here assume that independently and identically distributed (i.i.d.) measurement errors <span class="math notranslate nohighlight">\(e_i\)</span> that all follow a normal distribution with zero mean and variance <span class="math notranslate nohighlight">\(\sigma_e^2\)</span>. In a statistical notation we write <span class="math notranslate nohighlight">\(e_i \sim \mathcal{N}(0,\sigma_e^2)\)</span>. By default, we set <span class="math notranslate nohighlight">\(\sigma_e = 0.5\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">data_generating_process_measurement</span><span class="p">(</span><span class="n">process</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> 
                           <span class="n">sigma_error</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()):</span>
      
  <span class="n">ydata</span> <span class="o">=</span> <span class="n">process</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">xdata</span><span class="p">)</span>
  
  <span class="c1">#  sigma_error: measurement error. </span>
  <span class="n">error</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">sigma_error</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">xdata</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ydata</span><span class="o">+</span><span class="n">error</span><span class="p">,</span> <span class="n">sigma_error</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xdata</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us setup the data-generating process, in this case a linear process of polynomial degree 1, and decide how many measurements we make (<span class="math notranslate nohighlight">\(N_d=10\)</span>). All relevant output is stored in pandas dataframes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#the number of data points to collect</span>
<span class="c1"># -----</span>
<span class="n">Nd</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1"># -----</span>

<span class="c1"># predictor values</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="p">;</span> <span class="n">xmax</span> <span class="o">=</span> <span class="o">+</span><span class="mi">1</span>
<span class="n">Xmeasurement</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="n">Nd</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># store it in a pandas dataframe</span>
<span class="n">pd_Xmeasurement</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Xmeasurement</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="c1"># Define the data-generating process.</span>
<span class="c1"># Begin with a polynomial (poldeg=1) model_type</span>
<span class="c1"># in a second run of this notebook you can play with other linear models</span>
<span class="n">reality</span><span class="p">,</span> <span class="n">true_params</span><span class="p">,</span> <span class="n">model_type</span> <span class="o">=</span> <span class="n">data_generating_process_reality</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s1">&#39;polynomial&#39;</span><span class="p">,</span><span class="n">poldeg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;model type      : </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;true parameters : </span><span class="si">{</span><span class="n">true_params</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Nd = </span><span class="si">{</span><span class="n">Nd</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Collect measured data</span>
<span class="c1"># -----</span>
<span class="n">sigma_e</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="c1"># -----</span>
<span class="n">Ydata</span><span class="p">,</span> <span class="n">Yerror</span> <span class="o">=</span> <span class="n">data_generating_process_measurement</span><span class="p">(</span><span class="n">reality</span><span class="p">,</span><span class="n">true_params</span><span class="p">,</span><span class="n">Xmeasurement</span><span class="p">,</span><span class="n">sigma_error</span><span class="o">=</span><span class="n">sigma_e</span><span class="p">)</span>
<span class="c1"># store the data in a pandas dataframe</span>
<span class="n">pd_D</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Ydata</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="c1"># </span>
<span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Xmeasurement</span>
<span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Yerror</span>

<span class="c1"># We will also produce a denser grid for predictions with our model and comparison with the true process. This is useful for plotting</span>
   
<span class="n">xreality</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span><span class="n">xmax</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pd_R</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reality</span><span class="p">(</span><span class="n">true_params</span><span class="p">,</span><span class="n">xreality</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="n">pd_R</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">xreality</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>model type      : polynomial
true parameters : [ 3.37528642 -1.91692668]
Nd = 10
</pre></div>
</div>
</div>
</div>
<p>Create some analysis tool to inspect the data, and later on the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># helper function to plot data, reality, and model (pd_M)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_data</span><span class="p">(</span><span class="n">pd_D</span><span class="p">,</span> <span class="n">pd_R</span><span class="p">,</span> <span class="n">pd_M</span><span class="p">,</span> <span class="n">with_errorbars</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Data&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="n">with_errorbars</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">],</span> <span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">],</span><span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">);</span>
    <span class="k">if</span> <span class="n">pd_R</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pd_R</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">pd_R</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Reality&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pd_M</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pd_M</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">pd_M</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span><span class="n">zorder</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Collected data&#39;</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Predictor $x$&#39;</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Response $y$&#39;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span><span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at the data. We set the last two arguments to <code class="docutils literal notranslate"><span class="pre">None</span></code> for visualizing only the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_data</span><span class="p">(</span><span class="n">pd_D</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c85199e9db731a54fca7281ea2c6068bdaa681ca954806fe6be4ef9bcfe43009.png" src="../../_images/c85199e9db731a54fca7281ea2c6068bdaa681ca954806fe6be4ef9bcfe43009.png" />
</div>
</div>
<p>Linear regression proceeds via the design matrix. We will analyze this data using a linear polynomial model of order 1. The following code will allow you to setup the corresponding design matrix <span class="math notranslate nohighlight">\(\dmat\)</span> for any polynomial order (referred to as poldeg below)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">setup_polynomial_design_matrix</span><span class="p">(</span><span class="n">data_frame</span><span class="p">,</span> <span class="n">poldeg</span><span class="p">,</span> <span class="n">drop_constant</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;setting up design matrix:&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  len(data):&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_frame</span><span class="o">.</span><span class="n">index</span><span class="p">))</span>

        <span class="c1"># for polynomial models: x^0, x^1, x^2, ..., x^p</span>
        <span class="c1"># use numpy increasing vandermonde matrix</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  model poldeg:&#39;</span><span class="p">,</span><span class="n">poldeg</span><span class="p">)</span>
    
    <span class="n">predictors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">(</span><span class="n">data_frame</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">poldeg</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">increasing</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">drop_constant</span><span class="p">:</span>
        <span class="n">predictors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  dropping constant term&#39;</span><span class="p">)</span>
    <span class="n">pd_design_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predictors</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">pd_design_matrix</span>
</pre></div>
</div>
</div>
</div>
<p>So, let’s setup the design matrix for a model with polynomial basis functions. Note that there are <span class="math notranslate nohighlight">\(N_p\)</span> parameters in a polynomial function of order <span class="math notranslate nohighlight">\(N_p-1\)</span></p>
<div class="math notranslate nohighlight">
\[
M(\pars;\inputt) = \theta_0 + \theta_1 \inputt.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Np</span><span class="o">=</span><span class="mi">2</span>
<span class="n">pd_X</span> <span class="o">=</span> <span class="n">setup_polynomial_design_matrix</span><span class="p">(</span><span class="n">pd_Xmeasurement</span><span class="p">,</span><span class="n">poldeg</span><span class="o">=</span><span class="n">Np</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>setting up design matrix:
  len(data): 10
  model poldeg: 1
</pre></div>
</div>
</div>
</div>
<p>We can now perform linear regression, or ordinary least squares (OLS), as</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#ols estimator for physical parameter theta</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">pd_D</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd_X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="n">ols_cov</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">))</span>
<span class="n">ols_xTd</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>
<span class="n">ols_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ols_cov</span><span class="p">,</span><span class="n">ols_xTd</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Ndata = </span><span class="si">{</span><span class="n">Nd</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;theta_ols </span><span class="se">\t</span><span class="si">{</span><span class="n">ols_theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;theta_true </span><span class="se">\t</span><span class="si">{</span><span class="n">true_params</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ndata = 10
theta_ols 	[ 3.27117461 -1.73245406]
theta_true 	[ 3.37528642 -1.91692668]
</pre></div>
</div>
</div>
</div>
<p>To evaluate the (fitted) model we setup a design matrix that spans dense values across the relevant range of predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd_Xreality</span> <span class="o">=</span> <span class="n">setup_polynomial_design_matrix</span><span class="p">(</span><span class="n">pd_R</span><span class="p">,</span><span class="n">poldeg</span><span class="o">=</span><span class="n">Np</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>setting up design matrix:
  len(data): 200
  model poldeg: 1
</pre></div>
</div>
</div>
</div>
<p>and then we dot this with the fitted (ols) parameter values</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xreality</span> <span class="o">=</span> <span class="n">pd_Xreality</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">pd_M_ols</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Xreality</span><span class="p">,</span><span class="n">ols_theta</span><span class="p">),</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="n">pd_M_ols</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">xreality</span>
</pre></div>
</div>
</div>
</div>
<p>A plot (which now includes the data-generating process ‘reality’) demonstrates the quality of the inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_data</span><span class="p">(</span><span class="n">pd_D</span><span class="p">,</span> <span class="n">pd_R</span><span class="p">,</span> <span class="n">pd_M_ols</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/087a8e56fde83643e26c931f567b265bc5061aa488435d29271245b68e672815.png" src="../../_images/087a8e56fde83643e26c931f567b265bc5061aa488435d29271245b68e672815.png" />
</div>
</div>
<p>To conclude, we also compute the sample variance <span class="math notranslate nohighlight">\(s^2\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ols_D</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">ols_theta</span><span class="p">)</span>
<span class="n">ols_eps</span> <span class="o">=</span> <span class="p">(</span><span class="n">ols_D</span> <span class="o">-</span> <span class="n">D</span><span class="p">)</span>
<span class="n">ols_s2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ols_eps</span><span class="p">,</span><span class="n">ols_eps</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">Nd</span><span class="o">-</span><span class="n">Np</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;s^2       </span><span class="se">\t</span><span class="si">{</span><span class="n">ols_s2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;sigma_e^2 </span><span class="se">\t</span><span class="si">{</span><span class="n">sigma_e</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>s^2       	0.412
sigma_e^2 	0.250
</pre></div>
</div>
</div>
</div>
<p>As seen, the extracted variance is in some agreement with the true one.</p>
<p>Using the code above, you should now try to do the following exercises.</p>
<div class="exercise admonition" id="exercise:ols_example_4">

<p class="admonition-title"><span class="caption-number">Exercise 37.4 </span></p>
<section id="exercise-content">
<p>Keep working with the simple polynomial model <span class="math notranslate nohighlight">\(M = \theta_0 + \theta_1 x\)</span></p>
<p>Reduce the number of data to 2, i.e., set Nd=2. Do you reproduce the result from the simple example in the previous section?</p>
<p>Increase the number of data to 1000. Do the OLS values of the model parameters and the sample variance approach the (true) parameters of the data-generating process? Is this to be expected?</p>
</section>
</div>
<div class="exercise admonition" id="exercise:ols_example_5">

<p class="admonition-title"><span class="caption-number">Exercise 37.5 </span></p>
<section id="exercise-content">
<p>Set the data-generating process to be a 3rd-order polynomial and set limits of the the predictor variable to [-3,3]. Analyze the data using a 2nd-order polynomial model.</p>
<p>Explore the limit of <span class="math notranslate nohighlight">\(N_d \rightarrow \infty\)</span> by setting <span class="math notranslate nohighlight">\(N_d = 500\)</span> or so. Will the OLS values of the model parameters and the sample variance approach the (true) values for some of the parameters?</p>
</section>
</div>
</section>
<section id="solutions">
<h2><span class="section-number">37.5. </span>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h2>
<p>Here are answers and solutions to selected exercises.</p>
<div class="solution dropdown admonition" id="solution:ols_example_1">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:ols_example_1"> Exercise 37.1</a></p>
<section id="solution-content">
<p>We have the following design matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\dmat = \left[
    \begin{array}{cc}
        1 &amp; -2 \\
        1 &amp; 1
    \end{array}
\right],
\end{split}\]</div>
<p>which in the present case yields the parameter values</p>
<div class="math notranslate nohighlight">
\[
\pars^{*} = \dmat^{-1}\data = [1,2]^T.
\]</div>
</section>
</div>
<div class="solution dropdown admonition" id="solution:ols_example_3">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:ols_example_3"> Exercise 37.3</a></p>
<section id="solution-content">
<p>For the warmup case we have fitted a straight line through two data points, which is always possible, and we cannot determine the sample variance. This will be even more clear when we come to <a class="reference internal" href="../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html#sec-bayesianlinearregression"><span class="std std-ref">Bayesian Linear Regression (BLR)</span></a>.</p>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/ModelingOptimization"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="OverviewModeling.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">36. </span>Overview of modeling</p>
      </div>
    </a>
    <a class="right-next"
       href="MathematicalOptimization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">38. </span>Mathematical optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-of-linear-models">37.1. Definition of linear models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis-with-linear-models">37.2. Regression analysis with linear models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-equation">The normal equation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-linear-regression-warmup">37.3. Ordinary linear regression: warmup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-linear-regression-in-practice">37.4. Ordinary linear regression in practice</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">37.5. Solutions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian Forssén, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>