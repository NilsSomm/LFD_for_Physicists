
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7.1. Advantages of the Bayesian approach &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/BayesianStatistics/BayesianBasics/BayesianAdvantages';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="7.2. Bayesian research workflow" href="../BayesianWorkflow/BayesianWorkflow.html" />
    <link rel="prev" title="7. Bayes in practice" href="UsingBayes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Overview.html">1. Invitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="MoreBayesTheorem.html">4.7. More on Bayes’ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="Exploring_pdfs.html">5.1. Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="visualization_of_CLT.html">Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="chi_squared_tests.html">5.4. Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="demo-BayesianBasics.html">6.6. Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_updating_coinflip_interactive.html">6.7. Widgetized coin tossing</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="UsingBayes.html">7. Bayes in practice</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/dealing_with_outliers.html">9.5. Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Multimodel_inference.html">12. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ModelSelection/ModelSelection.html">12.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">13.4. Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../MachineLearning/RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/demo-GaussianProcesses.html">demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/LogReg/LogReg.html">21. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearningExamples.html">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">22.7. ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">23.3. Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">28. Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">29.5. SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">36. Overview of modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">37. Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">38. Mathematical optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">40. Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">41.5. Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">41.6. Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">41.7. Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_development" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_development/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/BayesianStatistics/BayesianBasics/BayesianAdvantages.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/BayesianStatistics/BayesianBasics/BayesianAdvantages.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Advantages of the Bayesian approach</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-with-parametric-models">Inference with parametric models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-i-nuisance-parameters-and-marginalization">Error propagation (I): Nuisance parameters and marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-ii-changing-variables">Error propagation (II): Changing variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-iii-a-useful-approximation">Error propagation (III): A useful approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="advantages-of-the-bayesian-approach">
<span id="sec-bayesianadvantages"></span><h1><span class="section-number">7.1. </span>Advantages of the Bayesian approach<a class="headerlink" href="#advantages-of-the-bayesian-approach" title="Link to this heading">#</a></h1>
<blockquote class="epigraph">
<div><blockquote>
<div><p>“Bayesian inference probabilities are a measure of our state of knowledge about nature, not a measure of nature itself.”</p>
</div></blockquote>
<p class="attribution">—Devinderjit Sivia </p>
</div></blockquote>
<p>The Bayesian approach offers a number of distinct advantages in scientific applications. Some of them are listed below. In this chapter we introduce in particular the important tasks of inference with parametric models and the propagation of errors.</p>
<div class="admonition-how-the-bayesian-approach-helps-in-science admonition">
<p class="admonition-title">How the Bayesian approach helps in science</p>
<ol class="arabic simple">
<li><p>Provides an elegantly simple and rational approach for answering any scientific question for a given state of information. The procedure is well-defined:</p>
<ul class="simple">
<li><p>Clearly state your question and prior information.</p></li>
<li><p>Apply the sum and product rules. The starting point is always Bayes’ theorem.</p></li>
</ul>
</li>
<li><p>Provides a way of eliminating nuisance parameters through marginalization.</p>
<ul class="simple">
<li><p>For some problems, the marginalization can be performed analytically, permitting certain calculations to become computationally tractable.</p></li>
</ul>
</li>
<li><p>Provides a well-defined procedure for propagating errors,</p>
<ul class="simple">
<li><p>E.g., incorporating the effects of systematic errors arising from both the measurement operation and theoretical model predictions.</p></li>
</ul>
</li>
<li><p>Incorporates relevant prior (e.g., known signal model or known theory model expansion) information through Bayes’ theorem.</p>
<ul class="simple">
<li><p>This is one of the great strengths of Bayesian analysis.</p></li>
<li><p>Enforces explicit assumptions.</p></li>
<li><p>For data with a small signal-to-noise ratio, a Bayesian analysis can frequently yield many orders of magnitude improvement in model parameter estimation, through the incorporation of relevant prior information about the signal model.</p></li>
</ul>
</li>
<li><p>For some problems, a Bayesian analysis may simply lead to a familiar statistic. Even in this situation it often provides a powerful new insight concerning the interpretation of the statistic.</p></li>
<li><p>Provides a more powerful way of assessing competing theories at the forefront of science by automatically quantifying Occam’s razor.</p>
<ul class="simple">
<li><p>The evidence for two hypotheses or models, <span class="math notranslate nohighlight">\(M_i\)</span> and <span class="math notranslate nohighlight">\(M_j\)</span>, can be compared in light of data <span class="math notranslate nohighlight">\(\data\)</span> by evaluating the ratio <span class="math notranslate nohighlight">\(p(M_i|\data, I) / (M_j|\data, I)\)</span>.</p></li>
<li><p>The Bayesian quantitative Occam’s razor can also save a lot of time that might otherwise be spent chasing noise artifacts that masquerade as possible detections of real phenomena.</p></li>
</ul>
</li>
</ol>
</div>
<div class="tip admonition">
<p class="admonition-title">Occam’s razor</p>
<p>Occam’s razor is a principle attributed to the medieval philosopher William of Occam (or Ockham). The principle states that one should not make more assumptions than the minimum needed. It underlies all scientific modeling and theory building. It cautions us to choose from a set of otherwise equivalent models of a given phenomenon the simplest one. In any given model, Occam’s razor helps us to “shave off” those variables that are not really needed to explain the phenomenon. It was previously thought to be only a qualitative principle.</p>
<figure class="align-default" id="fig-leprechaun">
<a class="reference internal image-reference" href="../../../_images/Leprechaun_or_Clurichaun.png"><img alt="../../../_images/Leprechaun_or_Clurichaun.png" src="../../../_images/Leprechaun_or_Clurichaun.png" style="height: 250px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 7.1 </span><span class="caption-text">Did the Leprechaun drink your wine, or is there a simpler explanation?</span><a class="headerlink" href="#fig-leprechaun" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
<section id="inference-with-parametric-models">
<h2>Inference with parametric models<a class="headerlink" href="#inference-with-parametric-models" title="Link to this heading">#</a></h2>
<p>Inductive inference with parametric models is a very important tool in the natural sciences.</p>
<ul class="simple">
<li><p>Consider <span class="math notranslate nohighlight">\(N\)</span> different models <span class="math notranslate nohighlight">\(M_i\)</span> (<span class="math notranslate nohighlight">\(i = 1, \ldots, N\)</span>), each with a parameter vector <span class="math notranslate nohighlight">\(\pars_i\)</span>. The number of parameters (length of <span class="math notranslate nohighlight">\(\pars_i\)</span>) might be different for different models. Each of them implies a sampling distribution for possible data</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-8f0fda66-cbeb-4102-a6b6-1930293c83b3">
<span class="eqno">(7.1)<a class="headerlink" href="#equation-8f0fda66-cbeb-4102-a6b6-1930293c83b3" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(\data|{\pars}_i, M_i)
\end{equation}\]</div>
<ul class="simple">
<li><p>The likelihood function is the pdf of the actual, observed data (<span class="math notranslate nohighlight">\(\data_\mathrm{obs}\)</span>) given a set of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_i\)</span>:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-e3ebce11-9369-41f5-81af-e425faaaf7ad">
<span class="eqno">(7.2)<a class="headerlink" href="#equation-e3ebce11-9369-41f5-81af-e425faaaf7ad" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\mathcal{L}}_i (\pars_i) \equiv p(\data_\mathrm{obs}|\pars_i, M_i)
\end{equation}\]</div>
<ul class="simple">
<li><p>We may be uncertain about <span class="math notranslate nohighlight">\(M_i\)</span> (model uncertainty),</p></li>
<li><p>or uncertain about <span class="math notranslate nohighlight">\(\pars_i\)</span> (parameter uncertainty).</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Parameter Estimation:</p>
<p>Premise: We have chosen a model (say <span class="math notranslate nohighlight">\(M_1\)</span>)</p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about its parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_1\)</span>?</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Model comparison:</p>
<p>Premise: We have a set of different models <span class="math notranslate nohighlight">\(\{M_i\}\)</span></p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> How do they compare with each other? Do we have evidence to say that, e.g. <span class="math notranslate nohighlight">\(M_1\)</span>, is better than <span class="math notranslate nohighlight">\(M_2\)</span>?</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Model checking:</p>
<p>Premise: We have a model <span class="math notranslate nohighlight">\(M_1\)</span></p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> Is <span class="math notranslate nohighlight">\(M_1\)</span> adequate?</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Hybrid Uncertainty:</p>
<p>Premise: Models share some common parameters: <span class="math notranslate nohighlight">\(\pars_i = \{ \boldsymbol{\varphi}, {\boldsymbol{\eta}}_i\}\)</span></p>
<p><span class="math notranslate nohighlight">\(\Rightarrow\)</span> What can we say about <span class="math notranslate nohighlight">\(\boldsymbol{\varphi}\)</span>? (Systematic error is an example)</p>
</div>
<p>Further discussion on parameter estimation and scientific model predictions will appear in subsequent chapters.</p>
<figure class="align-center" id="fig-m1m2">
<a class="reference internal image-reference" href="../../../_images/m1m2.png"><img alt="../../../_images/m1m2.png" src="../../../_images/m1m2.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 7.2 </span><span class="caption-text">Joint pdf for the masses of two black holes merging obtained from the data analysis of a gravitational wave signal. This representation of a joint pdf is known as a corner plot.</span><a class="headerlink" href="#fig-m1m2" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="error-propagation-i-nuisance-parameters-and-marginalization">
<h2>Error propagation (I): Nuisance parameters and marginalization<a class="headerlink" href="#error-propagation-i-nuisance-parameters-and-marginalization" title="Link to this heading">#</a></h2>
<p>The Bayesian approach offers a straightforward approach for dealing with (known) systematic uncertainties; namely by marginalization.</p>
<p>Assume that we have a model with two parameters, <span class="math notranslate nohighlight">\(\theta,\phi\)</span>, although only one of them (say <span class="math notranslate nohighlight">\(\theta\)</span>) is of physical relevance. The other one is then labeled a nuisance parameter. Through a Bayesian data analysis we can get the the joint posterior PDF</p>
<div class="amsmath math notranslate nohighlight" id="equation-0584317e-f8f1-4cb3-aeae-8ebce1973100">
<span class="eqno">(7.3)<a class="headerlink" href="#equation-0584317e-f8f1-4cb3-aeae-8ebce1973100" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{\theta, \phi}{\data, I}.
\end{equation}\]</div>
<p>The marginal posterior PDF <span class="math notranslate nohighlight">\(\pdf{\theta}{\data, I}\)</span> is obtained via marginalization</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e3b5859-35b7-45ac-bae8-2cb6a16b853a">
<span class="eqno">(7.4)<a class="headerlink" href="#equation-4e3b5859-35b7-45ac-bae8-2cb6a16b853a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{\theta}{\data, I} = \int \pdf{\theta, \phi}{\data, I} d\phi.
\end{equation}\]</div>
<p>This simple procedure allows to propagate the uncertainty in <span class="math notranslate nohighlight">\(\phi\)</span> to the probability distribution for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p><a class="reference internal" href="#example:BayesianAdvantage:inferring-galactic-distances">Example 7.2</a> provides an illustration of this scenario.</p>
<div class="tip admonition">
<p class="admonition-title">Marginalization using samples</p>
<p>Assume that we have <span class="math notranslate nohighlight">\(N\)</span> samples from the joint pdf <span class="math notranslate nohighlight">\(\pdf{\theta, \phi}{\data, I}\)</span>. This might a sample chain from an MCMC sampler: <span class="math notranslate nohighlight">\(\left\{ (\theta, \phi)_i \right\}_{i=0}^{N-1}\)</span>. Then the marginal distribution of <span class="math notranslate nohighlight">\(\theta\)</span> will be given by the same chain by simply ignoring the <span class="math notranslate nohighlight">\(\phi\)</span> column, i.e., <span class="math notranslate nohighlight">\(\left\{ \theta_{i} \right\}_{i=0}^{N-1}\)</span>.</p>
<p>See the interactive demos created by Chi Feng for an illustration of this: <a class="reference external" href="https://chi-feng.github.io/mcmc-demo/">The Markov-chain Monte Carlo Interactive Gallery</a>.</p>
</div>
<p>A slightly more general scenario for which the marginalization procedure is useful is the following: Assume that we have measured or inferred the parameters <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>; what can we say about the difference <span class="math notranslate nohighlight">\(X-Y\)</span> or the raio <span class="math notranslate nohighlight">\(X/Y\)</span>, or the sum of their squares <span class="math notranslate nohighlight">\(X^2+Y^2\)</span>, etc?</p>
<p>Such questions can be rephrased as: Given the joint PDF <span class="math notranslate nohighlight">\(\pdf{x,y}{I}\)</span>, what is <span class="math notranslate nohighlight">\(\pdf{z}{I}\)</span>, where <span class="math notranslate nohighlight">\(z=f(x,y)\)</span>? Here, and in the following, we use shorthands <span class="math notranslate nohighlight">\(\pdf{z}{I}\)</span>, <span class="math notranslate nohighlight">\(\pdf{x,y}{I}\)</span> instead of the more correct <span class="math notranslate nohighlight">\(p_Z(z|I)\)</span>, <span class="math notranslate nohighlight">\(p_{X,Y}(x,y|I)\)</span>. The context should make it clear which random variable(s) that are referred to.</p>
<p>In this situation we can use marginalization and the product rule.</p>
<div class="amsmath math notranslate nohighlight" id="equation-72825043-ef26-42f1-aea5-b1f67e7b983b">
<span class="eqno">(7.5)<a class="headerlink" href="#equation-72825043-ef26-42f1-aea5-b1f67e7b983b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{z}{I} = \int \pdf{z,x,y}{I}dxdy = \int \pdf{z}{x,y,I} \pdf{x,y}{I}dxdy.
\end{equation}\]</div>
<p>We realize that <span class="math notranslate nohighlight">\(\pdf{z}{x,y,I} = \delta(z-f(x,y))\)</span> due to the functional relationship between the parameters.</p>
<div class="admonition-dirac-delta-functions admonition">
<p class="admonition-title">Dirac delta functions</p>
<p>A delta function <span class="math notranslate nohighlight">\(\delta(x-x_0)\)</span> can be constructed as the limiting case of a distribution</p>
<div class="math notranslate nohighlight">
\[
\delta(x-x_0) = \lim_{\varepsilon \to 0^+} h_{\varepsilon}(x-x_0).
\]</div>
<p>For example, it can be constructed as an infinitely narrow (and tall) normal distribution</p>
<div class="math notranslate nohighlight">
\[
\delta(x-x_0) = \lim_{\varepsilon \to 0^+} \frac{1}{\sqrt{2\pi}\varepsilon} \exp\left( -\frac{(x-x_0)^2}{2\varepsilon^2}\right).
\]</div>
<p>This function will be zero for <span class="math notranslate nohighlight">\(x \neq x_0\)</span>, and goes to infinity at <span class="math notranslate nohighlight">\(x_0\)</span> in a way such that the integral <span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty} \delta(x-x_0) dx = 1\)</span>, which is a defining property.</p>
<p>More general, for well-behaved functions <span class="math notranslate nohighlight">\(f(x)\)</span>, we have <span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty} f(x) \delta(x-x_0) dx = f(x_0)\)</span>.</p>
</div>
<p>The joint PDF for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> becomes a product if the errors are independent <span class="math notranslate nohighlight">\(\pdf{x,y}{I} = \pdf{x}{I} \pdf{y}{I}\)</span> . The delta function can be used to evaluate one of the integrals, giving some inverse transformation <span class="math notranslate nohighlight">\(y=g(x,z)\)</span>, and the PDF for <span class="math notranslate nohighlight">\(Z\)</span> becomes a convolution</p>
<div class="math notranslate nohighlight" id="equation-eq-bayesianadvantage-marginalization">
<span class="eqno">(7.6)<a class="headerlink" href="#equation-eq-bayesianadvantage-marginalization" title="Link to this equation">#</a></span>\[
\pdf{z}{I} = \int \pdf{x}{I} \pdf{y=g(x,z)}{I}dx.
\]</div>
<div class="proof example admonition" id="example:BayesianAdvantage:Z=X+Y">
<p class="admonition-title"><span class="caption-number">Example 7.1 </span> (<span class="math notranslate nohighlight">\(Z = X + Y\)</span>)</p>
<section class="example-content" id="proof-content">
<p>Let us consider the situation where you are interested in the quantity <span class="math notranslate nohighlight">\(Z = X + Y\)</span>, and you have information <span class="math notranslate nohighlight">\(I\)</span> about <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> which tells you that <span class="math notranslate nohighlight">\(\expect{X} = x_0\)</span>, <span class="math notranslate nohighlight">\(\var{X} = \sigma_x^2\)</span> and <span class="math notranslate nohighlight">\(\expect{Y} = y_0\)</span>, <span class="math notranslate nohighlight">\(\var{Y} = \sigma_y^2\)</span>. If this is all the information that we have, it is reasonable to assume that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent and that we should assign Gaussian  PDFs (the argument for this is known as the Maximum Entropy principle)</p>
<div class="amsmath math notranslate nohighlight" id="equation-cf61f094-9248-4806-9b9a-f5938fe0d371">
<span class="eqno">(7.7)<a class="headerlink" href="#equation-cf61f094-9248-4806-9b9a-f5938fe0d371" title="Permalink to this equation">#</a></span>\[\begin{align}
\pdf{x,y}{I} &amp;= \pdf{x}{I}\pdf{y}{I} \\
&amp;= \frac{1}{2\pi\sigma_x\sigma_y} \exp\left[ - \frac{(x-x_0)^2}{2\sigma_x^2} \right] \exp\left[ - \frac{(y-y_0)^2}{2\sigma_y^2} \right].
\end{align}\]</div>
<p>Let us now use marginalization and the product rule to find</p>
<div class="amsmath math notranslate nohighlight" id="equation-f306245c-0e4b-481b-b2fa-e50274ca77e8">
<span class="eqno">(7.8)<a class="headerlink" href="#equation-f306245c-0e4b-481b-b2fa-e50274ca77e8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{z}{I} = \int \pdf{z,x,y}{I}dxdy = \int \pdf{z}{x,y,I} \pdf{x,y}{I}dxdy.
\end{equation}\]</div>
<p>We realize that <span class="math notranslate nohighlight">\(\pdf{z}{x,y,I} = \delta(z-(x+y))\)</span> due to the functional relationship between the parameters, and we have <span class="math notranslate nohighlight">\(\pdf{x,y}{I}\)</span> as a product of Gaussian  PDFs from above. The delta function can be used to evaluate one of the integrals, and the PDF for <span class="math notranslate nohighlight">\(Z\)</span> becomes a convolution</p>
<div class="amsmath math notranslate nohighlight" id="equation-63b51cb6-d0f8-42c9-9e8b-ffda721f987d">
<span class="eqno">(7.9)<a class="headerlink" href="#equation-63b51cb6-d0f8-42c9-9e8b-ffda721f987d" title="Permalink to this equation">#</a></span>\[\begin{align}
\pdf{z}{I} &amp;= \int \pdf{x}{I} \pdf{y=z-x}{I} dx \\
&amp;= \frac{1}{2\pi\sigma_x\sigma_y} \int \exp\left[ - \frac{(x-x_0)^2}{2\sigma_x^2} \right] \exp\left[ - \frac{(z - x - y_0)^2}{2\sigma_y^2} \right] dx.
\end{align}\]</div>
<p>After some tedious algebra that involves completing the square for <span class="math notranslate nohighlight">\(X\)</span> in the exponent we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-bayesianadvantage-sum-of-gaussians">
<span class="eqno">(7.10)<a class="headerlink" href="#equation-eq-bayesianadvantage-sum-of-gaussians" title="Link to this equation">#</a></span>\[
\pdf{z}{I} = \frac{1}{\sqrt{2\pi}\sigma_z} \exp\left[ - \frac{(z-z_0)^2}{2\sigma_z^2} \right], 
\]</div>
<p>with <span class="math notranslate nohighlight">\(z_0 = x_0 + y_0\)</span> and <span class="math notranslate nohighlight">\(\sigma_z^2 = \sigma_x^2 + \sigma_y^2\)</span>. Thus, the PDF for the sum <span class="math notranslate nohighlight">\(Z=X+Y\)</span>, with <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> being described by Gaussian  PDFs, is another Gaussian.</p>
</section>
</div><div class="exercise admonition" id="exercise:BayesianAdvantage:correlated-errors">

<p class="admonition-title"><span class="caption-number">Exercise 7.1 </span> (Correlated errors)</p>
<section id="exercise-content">
<p>For correlated errors the joint PDF <span class="math notranslate nohighlight">\(\pdf{x,y}{I}\)</span> does not factorize into a product. Consider, for example, a situation with a bivariate normal distribution</p>
<div class="math notranslate nohighlight">
\[
\pdf{x,y}{I} = \frac{1}{(2\pi) |\boldsymbol{\Sigma}|^{1/2}} \exp{ \left( -\frac{1}{2}(x, y)\boldsymbol{\Sigma}^{-1}(x, y)^T\right)}.
\]</div>
<p>Furthermore, let us assume that the variances in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are the same (<span class="math notranslate nohighlight">\(\sigma_x^2=\sigma_y^2\equiv\sigma^2\)</span>) and the covariance is positive and almost as large as the variance (i.e., errors are strongly correlated). Then, the linear transformation <span class="math notranslate nohighlight">\(S=(X-Y)/\sqrt{2}\)</span> and <span class="math notranslate nohighlight">\(T=(X+Y)/\sqrt{2}\)</span> gives two new random variables <span class="math notranslate nohighlight">\(S\)</span> and <span class="math notranslate nohighlight">\(T\)</span> that turn out to be independent. This implies</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pdf{x,y}{I} = 
\left\{ 
\begin{array}{l}
s = \frac{x-y}{\sqrt{2}} \\
t = \frac{x+y}{\sqrt{2}} \\
\end{array}
\right\}
= \frac{1}{\sqrt{2\pi}\sigma_s}\exp \left( - \frac{s^2}{2\sigma_s^2} \right) \frac{1}{\sqrt{2\pi}\sigma_t}\exp \left( - \frac{t^2}{2\sigma_t^2} \right).
\end{split}\]</div>
<p>Furthermore, given the strong positive correlation one finds that <span class="math notranslate nohighlight">\(\sigma_s \ll \sigma\)</span>. What is the PDF for the difference <span class="math notranslate nohighlight">\(Z=X-Y\)</span> in this situation?</p>
</section>
</div>
<div class="exercise admonition" id="exercise:BayesianAdvantage:complete-the square">

<p class="admonition-title"><span class="caption-number">Exercise 7.2 </span> (Sum of two Gaussian  PDFs)</p>
<section id="exercise-content">
<p>Complete the derivation of Eq. <a class="reference internal" href="#equation-eq-bayesianadvantage-sum-of-gaussians">(7.10)</a>.</p>
</section>
</div>
<div class="proof example admonition" id="example:BayesianAdvantage:inferring-galactic-distances">
<p class="admonition-title"><span class="caption-number">Example 7.2 </span> (Inferring galactic distances)</p>
<section class="example-content" id="proof-content">
<p>The Hubble constant <span class="math notranslate nohighlight">\(H\)</span> acts as a galactic ruler as it is used to measure astronomical distances (<span class="math notranslate nohighlight">\(d\)</span>) according to <span class="math notranslate nohighlight">\(v = H d\)</span> where <span class="math notranslate nohighlight">\(v\)</span> is the velocity that can be measured with observations of redshifts. An error in <span class="math notranslate nohighlight">\(H\)</span> will therefore correspond to a systematic uncertainty in such measurements.</p>
<p>A modern value for the Hubble constant is <span class="math notranslate nohighlight">\(H = 70 \pm 10\)</span> km s<span class="math notranslate nohighlight">\(^{-1}\)</span> MPc<span class="math notranslate nohighlight">\(^{-1}\)</span>. We introduce the expectation value <span class="math notranslate nohighlight">\(\expect{H} = H_0 = 70\)</span> km s<span class="math notranslate nohighlight">\(^{-1}\)</span> MPc<span class="math notranslate nohighlight">\(^{-1}\)</span> and the standard deviation <span class="math notranslate nohighlight">\(\std{H} = \sigma_H = 10\)</span> km s<span class="math notranslate nohighlight">\(^{-1}\)</span> MPc<span class="math notranslate nohighlight">\(^{-1}\)</span>. Note that astronomical distances are usually measured in million parsecs (MPc). Suppose that a specific galaxy has a measured recessional velocity <span class="math notranslate nohighlight">\(v_0 = 100 \times 10^3\)</span> km/s with an observational error quantified by a standard deviation <span class="math notranslate nohighlight">\(\sigma_v = 5 \times 10^3\)</span> km/s. Determine the posterior PDF for the distance to a galaxy under the following analysis assumptions:</p>
<ol class="arabic">
<li><p>We use the expectation value of <span class="math notranslate nohighlight">\(H\)</span> in the analysis: <span class="math notranslate nohighlight">\(H = H_0 = 70\)</span> km s<span class="math notranslate nohighlight">\(^{-1}\)</span> MPc<span class="math notranslate nohighlight">\(^{-1}\)</span>.</p></li>
<li><p>We include the uncertainty in the value of the Hubble constant via a Gaussian PDF:</p>
<div class="math notranslate nohighlight">
\[
   \pdf{H}{I} = \frac{1}{\sqrt{2\pi}\sigma_H} \exp\left( - \frac{(H-H_0)^2}{2 \sigma_H^2} \right).
   \]</div>
</li>
<li><p>We include the uncertainty in the value of the Hubble constant via a uniform PDF:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \pdf{H}{I} = \left\{ 
   \begin{array}{ll}
   \frac{1}{4 \sigma_H} &amp; \text{for } H_0 - 2\sigma_H \leq H \leq H_0 + 2\sigma_H, \\
   0 &amp; \text{otherwise.}
   \end{array}
   \right.
   \end{split}\]</div>
</li>
<li><p>In addition, an approximate propagation of errors is demonstrated in <a class="reference internal" href="#example:BayesianAdvantage:inferring-galactic-distances-revisited">Example 7.3</a>.</p></li>
</ol>
<p>Here we use marginalization to obtain the desired posterior PDF <span class="math notranslate nohighlight">\(\pdf{d}{\data,I}\)</span> from the joint distribution of <span class="math notranslate nohighlight">\(\pdf{d,H}{\data,I}\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-marginalization-hubble">
<span class="eqno">(7.11)<a class="headerlink" href="#equation-eq-marginalization-hubble" title="Link to this equation">#</a></span>\[
\pdf{d}{\data,I} = \int_{-\infty}^\infty \pdf{d,H}{\data,I} dH .
\]</div>
<p>Using</p>
<ul class="simple">
<li><p>Bayes’ rule: <span class="math notranslate nohighlight">\(\pdf{d,H}{\data,I} \propto \pdf{\data}{d,H,I} \pdf{d,H}{I}\)</span>,</p></li>
<li><p>the product rule: <span class="math notranslate nohighlight">\(\pdf{d,H}{I} = p(H|d,I)\pdf{d}{I}\)</span>,</p></li>
<li><p>and the fact that <span class="math notranslate nohighlight">\(H\)</span> is independent of <span class="math notranslate nohighlight">\(d\)</span>: <span class="math notranslate nohighlight">\(p(H|d,I) = \pdf{H}{I}\)</span>,</p></li>
</ul>
<p>we find that</p>
<div class="amsmath math notranslate nohighlight" id="equation-fd710010-68f2-4125-9ef6-18029096eb51">
<span class="eqno">(7.12)<a class="headerlink" href="#equation-fd710010-68f2-4125-9ef6-18029096eb51" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{d}{\data,I} \propto \pdf{d}{I} \int \pdf{H}{I} \pdf{\data}{d,H,I} dH ,
\end{equation}\]</div>
<p>which means that we have expressed the quantity that we want (the posterior of <span class="math notranslate nohighlight">\(d\)</span>) in terms of quantities that we can specify.</p>
<p>First, we need to state our prior knowledge concerning the distance <span class="math notranslate nohighlight">\(\pdf{d}{I}\)</span>. You might ask yourself what information is contained in this quantity. It should summarize our state of knowledge <em>before</em> accumulating the new data. For starters, we do know that it is a distance (meaning that it is a positive quantity) and we would also expect it to be smaller than the (visible) size of the universe. We might argue that it should be at least as far away as the previously known closest galaxy. Further arguments can be made on the basis of indifference, but we will defer such discussions for now. For simplicity, we just assign a uniform PDF</p>
<div class="math notranslate nohighlight">
\[
\pdf{d}{I} \propto 1,
\]</div>
<p>within some huge range <span class="math notranslate nohighlight">\(0 &lt; d  &lt; 10^9\)</span> MPc.</p>
<p>The likelihood for the observed can be written down from a statistical model</p>
<div class="math notranslate nohighlight">
\[
v_\mathrm{measured} = H d + e,
\]</div>
<p>where <span class="math notranslate nohighlight">\(H d\)</span> represents our model for the velocity and <span class="math notranslate nohighlight">\(e\)</span> is the observational error, which is a random variable. We know that <span class="math notranslate nohighlight">\(\expect{v_\mathrm{measured}} = v_0\)</span> and <span class="math notranslate nohighlight">\(\text{Var}(v_\mathrm{measured}) = \sigma_v^2\)</span>. Given this information it is motivated to use a Gaussian PDF (with this mean and variance) for the likelihood</p>
<div class="math notranslate nohighlight">
\[
\pdf{\data}{d,H,I} = \frac{1}{\sqrt{2\pi}\sigma_v} \exp\left( - \frac{(v_0 - Hd)^2}{2\sigma_v^2} \right).
\]</div>
<p>The results for the different analysis strategies (1-3 in the list above) should be obtained in <a class="reference internal" href="#exercise:BayesianAdvantages:inferring-galactic-distances-ex"><span class="std std-numref">Exercise 7.3</span></a>. A comparison figure that also includes the approximative error propagation (4) is shown in the solution to this exercise.</p>
</section>
</div><div class="exercise admonition" id="exercise:BayesianAdvantages:inferring-galactic-distances-ex">

<p class="admonition-title"><span class="caption-number">Exercise 7.3 </span> (Inferring galactic distances)</p>
<section id="exercise-content">
<p>Apply the first three analysis strategies and derive the posterior distribution <span class="math notranslate nohighlight">\(\pdf{d}{\data,I}\)</span>. The first case allows an analytical solution while the second and third are probably best approached numerically (although it is, in fact, possible to find an analytical result also for case 2). Plot the results.</p>
<p>Hint 1: Since the goal is to extract a probability distribution, and we are already ignoring the denominator in Bayes’ theorem, it is allowed to ignore all normalization constants. The final  PDFs can be normalized by integration.</p>
<p>Hint 2: A fixed value for <span class="math notranslate nohighlight">\(H\)</span> can be assigned with the PDF <span class="math notranslate nohighlight">\(\pdf{H}{I} = \delta(H-H_0)\)</span>, where <span class="math notranslate nohighlight">\(\delta(x)\)</span> is the Kronecker delta.</p>
</section>
</div>
</section>
<section id="error-propagation-ii-changing-variables">
<span id="sec-bayesianadvantages-changingvariables"></span><h2>Error propagation (II): Changing variables<a class="headerlink" href="#error-propagation-ii-changing-variables" title="Link to this heading">#</a></h2>
<p>Let us consider a single variable <span class="math notranslate nohighlight">\(X\)</span> and a function <span class="math notranslate nohighlight">\(Y=f(X)\)</span> that offers a unique mapping between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Assume that we know <span class="math notranslate nohighlight">\(X\)</span> via a PDF <span class="math notranslate nohighlight">\(\pdf{x}{I}\)</span>. What is the relation between <span class="math notranslate nohighlight">\(\pdf{x}{I}\)</span> and <span class="math notranslate nohighlight">\(\pdf{y}{I}\)</span>? In this scenario the extraction of <span class="math notranslate nohighlight">\(\pdf{y}{I}\)</span> turns out to be an exercise in transformation of variables.</p>
<p>Consider a point <span class="math notranslate nohighlight">\(x^*\)</span> and a small interval <span class="math notranslate nohighlight">\(\delta x\)</span> around it. The probability that <span class="math notranslate nohighlight">\(X\)</span> lies within that interval can be written</p>
<div class="amsmath math notranslate nohighlight" id="equation-473c96a6-a772-46d9-bc5d-ca87776c6790">
<span class="eqno">(7.13)<a class="headerlink" href="#equation-473c96a6-a772-46d9-bc5d-ca87776c6790" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{x^* - \frac{\delta x}{2} \le x &lt; x^* + \frac{\delta x}{2}}{I}
\approx \pdf{x=x^*}{I} \delta x.
\end{equation}\]</div>
<p>Assume now, as stated above, that the function <span class="math notranslate nohighlight">\(f\)</span> maps the point <span class="math notranslate nohighlight">\(x=x^*\)</span> uniquely onto <span class="math notranslate nohighlight">\(y=y^*=f(x^*)\)</span>. Then there must be an interval <span class="math notranslate nohighlight">\(\delta y\)</span> around <span class="math notranslate nohighlight">\(y^*\)</span> so that the probability is conserved</p>
<div class="amsmath math notranslate nohighlight" id="equation-42cf7d2f-26e0-45a1-be6a-df90e9ade5a9">
<span class="eqno">(7.14)<a class="headerlink" href="#equation-42cf7d2f-26e0-45a1-be6a-df90e9ade5a9" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{x=x^*}{I} \delta x = \pdf{y=y^*}{I} \delta y.
\end{equation}\]</div>
<p>In the limit of infinitesimally small intervals, and with the realization that this should be true for any point <span class="math notranslate nohighlight">\(x\)</span>, we obtain the relationship</p>
<div class="math notranslate nohighlight" id="equation-eq-bayesianadvantage-transformation">
<span class="eqno">(7.15)<a class="headerlink" href="#equation-eq-bayesianadvantage-transformation" title="Link to this equation">#</a></span>\[
\pdf{x}{I} = p(y=y(x)|I) \left| \frac{dy}{dx} \right|,
\]</div>
<p>where the term on the far right is called the <em>Jacobian</em>. We also note that we can inverse the transformation</p>
<div class="math notranslate nohighlight" id="equation-eq-bayesianadvantage-inverse-transformation">
<span class="eqno">(7.16)<a class="headerlink" href="#equation-eq-bayesianadvantage-inverse-transformation" title="Link to this equation">#</a></span>\[
\pdf{y}{I} = \pdf{x(y)}{I} \left| \frac{dx}{dy} \right|,
\]</div>
<p>The generalization to several variables, relating the PDF for <span class="math notranslate nohighlight">\(M\)</span> variables <span class="math notranslate nohighlight">\(\{ Xx_j \}\)</span> in terms of the same number of quantities <span class="math notranslate nohighlight">\(\{ y_j \}\)</span> related to them, is</p>
<div class="math notranslate nohighlight" id="equation-eq-bayesianadvantage-multivariate-transformation">
<span class="eqno">(7.17)<a class="headerlink" href="#equation-eq-bayesianadvantage-multivariate-transformation" title="Link to this equation">#</a></span>\[
p(\{x_j\}|I) = p(\{y_j\}|I) \left| \frac{\partial (y_1, y_2, \ldots, y_M)}{\partial (x_1, x_2, \ldots, x_M)} \right|,
\]</div>
<p>where the multivariate Jacobian is given by the determinant of the <span class="math notranslate nohighlight">\(M \times M\)</span> matrix of partial derivatives <span class="math notranslate nohighlight">\(\partial y_i / \partial x_j\)</span>.</p>
<div class="exercise admonition" id="exercise:BayesianAdvantages:standard-random-variable">

<p class="admonition-title"><span class="caption-number">Exercise 7.4 </span> (The standard random variable)</p>
<section id="exercise-content">
<p>Find <span class="math notranslate nohighlight">\(\pdf{z}{I}\)</span> when <span class="math notranslate nohighlight">\(Z = (X-\mu)/\sigma\)</span> and <span class="math notranslate nohighlight">\(\pdf{x}{I} = \frac{1}{\sqrt{2\pi}\sigma} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right)\)</span>.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:BayesianAdvantages:square-root-of-a-number">

<p class="admonition-title"><span class="caption-number">Exercise 7.5 </span> (The square root of a number)</p>
<section id="exercise-content">
<p>Find an expression for <span class="math notranslate nohighlight">\(\pdf{z}{I}\)</span> when <span class="math notranslate nohighlight">\(Z = \sqrt{X}\)</span> and <span class="math notranslate nohighlight">\(\pdf{x}{I} = \frac{1}{x_{\max} - x_{\min}}\)</span> for <span class="math notranslate nohighlight">\(x_{\min} \leq x \leq x_{\max}\)</span> and 0 elsewhere. Verify that <span class="math notranslate nohighlight">\(\pdf{z}{I}\)</span> is properly normalized.</p>
</section>
</div>
<div class="tip admonition">
<p class="admonition-title">Summary</p>
<p>We have now seen the basic ingredients required for the propagation of errors: it either involves a transformation in the sense of Eq. <a class="reference internal" href="#equation-eq-bayesianadvantage-multivariate-transformation">(7.17)</a> or an integration as in Eq. <a class="reference internal" href="#equation-eq-bayesianadvantage-marginalization">(7.6)</a>.</p>
</div>
</section>
<section id="error-propagation-iii-a-useful-approximation">
<h2>Error propagation (III): A useful approximation<a class="headerlink" href="#error-propagation-iii-a-useful-approximation" title="Link to this heading">#</a></h2>
<p>For practical purposes, we are often satisfied to approximate PDFs with Gaussians. Within such limits there is an easier method that is often used for error propagation. Note, however, that there are instances when this method fails miserably as will be shown in the example further down.</p>
<p>Suppose that we have summarized the PDFs <span class="math notranslate nohighlight">\(\pdf{x}{I}\)</span> and <span class="math notranslate nohighlight">\(\pdf{y}{I}\)</span> as two Gaussians with mean and standard deviation <span class="math notranslate nohighlight">\(x_0, \sigma_x\)</span> and <span class="math notranslate nohighlight">\(y_0, \sigma_y\)</span>, respectively. Assume further that these two variables are not correlated, i.e., <span class="math notranslate nohighlight">\(\pdf{x,y}{I} = \pdf{x}{I} \pdf{y}{I}\)</span>.</p>
<p>Suppose now that we are interested in <span class="math notranslate nohighlight">\(Z=X-Y\)</span>. Intuitively, we might guess that the best estimate <span class="math notranslate nohighlight">\(z_0 = x_0 - y_0\)</span>, but the standard deviation <span class="math notranslate nohighlight">\(\sigma_z\)</span> requires some more thought. Differentiate the relation</p>
<div class="amsmath math notranslate nohighlight" id="equation-a1f17bcc-63df-4bb1-bc85-46445c3de579">
<span class="eqno">(7.18)<a class="headerlink" href="#equation-a1f17bcc-63df-4bb1-bc85-46445c3de579" title="Permalink to this equation">#</a></span>\[\begin{equation}
\delta Z = \delta X - \delta Y.
\end{equation}\]</div>
<p>For error bars, we are interested in this relation around the optimum, i.e., <span class="math notranslate nohighlight">\(\delta X = X - x_0\)</span> and so on. Square both sides and integrate to get the expectation value</p>
<div class="amsmath math notranslate nohighlight" id="equation-025df842-6c10-4f29-837b-cb8eb62dfbe1">
<span class="eqno">(7.19)<a class="headerlink" href="#equation-025df842-6c10-4f29-837b-cb8eb62dfbe1" title="Permalink to this equation">#</a></span>\[\begin{equation}
\expect{\delta Z^2} = \expect{\delta X^2 + \delta Y^2 - 2 \delta X \delta Y}  = \expect{\delta X^2} + \expect {\delta Y^2} - 2 \expect{\delta X \delta Y},
\end{equation}\]</div>
<p>where we have employed the linear property for an integral over a sum of terms.</p>
<p>Since we study the differential relation around the optimum, and we assumed that the PDFs for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> were described by independent Gaussians, we get</p>
<div class="math notranslate nohighlight" id="equation-eq-stddev">
<span class="eqno">(7.20)<a class="headerlink" href="#equation-eq-stddev" title="Link to this equation">#</a></span>\[
\expect{\delta X^2} = \sigma_x^2; \qquad \expect{\delta Y^2} = \sigma_y^2; \qquad \expect{\delta X \delta Y} = 0,
\]</div>
<p>and we find that</p>
<div class="amsmath math notranslate nohighlight" id="equation-8162c218-ae5d-4a56-8aae-7bbb2951b1bb">
<span class="eqno">(7.21)<a class="headerlink" href="#equation-8162c218-ae5d-4a56-8aae-7bbb2951b1bb" title="Permalink to this equation">#</a></span>\[\begin{equation}
\sigma_z = \sqrt{ \expect{\delta Z^2} } = \sqrt{ \sigma_x^2 + \sigma_y^2 }.
\end{equation}\]</div>
<div class="proof example admonition" id="example:BayesianAdvantage:inferring-galactic-distances-revisited">
<p class="admonition-title"><span class="caption-number">Example 7.3 </span> (Inferring galactic distances—revisited)</p>
<section class="example-content" id="proof-content">
<p>Consider, as a second example, the ratio of two parameters <span class="math notranslate nohighlight">\(Z = X/Y\)</span> that appeared in <a class="reference internal" href="#example:BayesianAdvantage:inferring-galactic-distances">Example 7.2</a> (in which we wanted to infer <span class="math notranslate nohighlight">\(x = v/H\)</span>). Differentiation gives</p>
<div class="amsmath math notranslate nohighlight" id="equation-f4024dc6-efec-4d65-87b1-205c48a33dd9">
<span class="eqno">(7.22)<a class="headerlink" href="#equation-f4024dc6-efec-4d65-87b1-205c48a33dd9" title="Permalink to this equation">#</a></span>\[\begin{equation}
\delta Z = \frac{Y \delta X - X \delta Y}{Y^2} \quad \Leftrightarrow \quad \frac{\delta Z}{Z} = \frac{\delta X}{X} - \frac{\delta Y}{Y}.
\end{equation}\]</div>
<p>Squaring both sides and taking the expectation values, we obtain</p>
<div class="amsmath math notranslate nohighlight" id="equation-e3b6b626-82c2-4ba6-bf62-601d51166a55">
<span class="eqno">(7.23)<a class="headerlink" href="#equation-e3b6b626-82c2-4ba6-bf62-601d51166a55" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{\expect{\delta Z^2}}{z_0^2} = \frac{\expect{\delta X^2}}{x_0^2} + \frac{\expect{\delta Y^2}}{y_0^2} - 2 \frac{\expect{\delta X} \expect{\delta Y}}{x_0 y_0},
\end{equation}\]</div>
<p>where the <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> in the denominator have been replaced by the constants <span class="math notranslate nohighlight">\(x_0\)</span>, <span class="math notranslate nohighlight">\(y_0\)</span> and <span class="math notranslate nohighlight">\(z_0 = x_0 / y_0\)</span> because we are interested in deviations from the peak of the PDF.</p>
<p>Finally, substituting the information for the PDFs of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as summarized in Eq. <a class="reference internal" href="#equation-eq-stddev">(7.20)</a> we finally obtain the propagated error for the ratio</p>
<div class="amsmath math notranslate nohighlight" id="equation-72e1c941-6ae0-4310-a1e7-b2e0ca1da404">
<span class="eqno">(7.24)<a class="headerlink" href="#equation-72e1c941-6ae0-4310-a1e7-b2e0ca1da404" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{\sigma_z}{z_0} = \sqrt{ \left( \frac{\sigma_x}{x_0} \right)^2 + \left( \frac{\sigma_y}{y_0} \right)^2}.
\end{equation}\]</div>
</section>
</div><div class="exercise admonition" id="exercise:BayesianAdvantages:gaussian-sum-of-errors">

<p class="admonition-title"><span class="caption-number">Exercise 7.6 </span> (Gaussian sum of errors)</p>
<section id="exercise-content">
<p>Consider <span class="math notranslate nohighlight">\(Z=X+Y\)</span> (again) and derive a PDF for <span class="math notranslate nohighlight">\(Z\)</span> assuming Gaussian errors in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Compare with the result from the full convolution of  PDFs in <a class="reference internal" href="#example:BayesianAdvantage:Z=X+Y">Example 7.1</a>.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:BayesianAdvantages:gaussian-product-of-errors">

<p class="admonition-title"><span class="caption-number">Exercise 7.7 </span> (Gaussian product of errors)</p>
<section id="exercise-content">
<p>Consider <span class="math notranslate nohighlight">\(Z=XY\)</span> and derive a PDF for <span class="math notranslate nohighlight">\(Z\)</span> assuming Gaussian errors in <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</section>
</div>
<p>Despite its virtues, let us end our discussion of error-propagation with a salutary warning against the blind use of this nifty short cut.</p>
<div class="proof example admonition" id="example:BayesianAdvantage:taking-square-root">
<p class="admonition-title"><span class="caption-number">Example 7.4 </span> (Taking the square root of a number)</p>
<section class="example-content" id="proof-content">
<ul class="simple">
<li><p>Assume that the amplitude of a Bragg peak is measured with an uncertainty <span class="math notranslate nohighlight">\(A = A_0 \pm \sigma_A\)</span> from a least-squares fit to experimental data.</p></li>
<li><p>The Bragg peak amplitude is proportional to the square of a complex structure function: <span class="math notranslate nohighlight">\(A = |F|^2 \equiv f^2\)</span>.</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(f = f_0 \pm \sigma_f\)</span>?</p></li>
</ul>
<p>Obviously, we have that <span class="math notranslate nohighlight">\(f_0 = \sqrt{A_0}\)</span>. Differentiate the relation, square and take the expectation value</p>
<div class="amsmath math notranslate nohighlight" id="equation-a79a0725-3931-413d-bd86-c41390013af4">
<span class="eqno">(7.25)<a class="headerlink" href="#equation-a79a0725-3931-413d-bd86-c41390013af4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\expect{\delta A^2} = 4 f_0^2 \expect{\delta f^2} \quad 
\Leftrightarrow \quad 
\sigma_f = \frac{\sigma_A}{2 \sqrt{A_0}},
\end{equation}\]</div>
<p>where we have used the Gaussian approximation for the PDFs.</p>
<p>But what happens if the best fit gives <span class="math notranslate nohighlight">\(A_0 &lt; 0\)</span>, which would not be impossible if we have weak and strongly overlapping peaks. The above equation obviously does not work since <span class="math notranslate nohighlight">\(f_0\)</span> would be a complex number.</p>
<p>We have made two mistakes:</p>
<ol class="arabic simple">
<li><p>Likelihood is not posterior!</p></li>
<li><p>The Gaussian approximation around the peak does not always work.</p></li>
</ol>
<p>Consider first the best fit of the signal peak. It implies that the likelihood can be approximated by</p>
<div class="amsmath math notranslate nohighlight" id="equation-d52eb200-3ceb-4de3-909d-4f92454d1fb8">
<span class="eqno">(7.26)<a class="headerlink" href="#equation-d52eb200-3ceb-4de3-909d-4f92454d1fb8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{\data}{A,I} \propto \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right].
\end{equation}\]</div>
<p>However, the posterior for <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\pdf{A}{{\data},I} \propto \pdf{\data}{A,I} \pdf{A}{I}\)</span> and we should use the fact that we know that <span class="math notranslate nohighlight">\(A \ge 0\)</span>.</p>
<p>We will incorporate this information through a simple step-function prior</p>
<div class="amsmath math notranslate nohighlight" id="equation-c4a5223d-098a-4e01-971a-a0d53621e4f0">
<span class="eqno">(7.27)<a class="headerlink" href="#equation-c4a5223d-098a-4e01-971a-a0d53621e4f0" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{A}{I} = \left\{
\begin{array}{ll}
\frac{1}{A_\mathrm{max}}, &amp; 0 \le A \le A_\mathrm{max}, \\
0, &amp; \mathrm{otherwise}.
\end{array}
\right.
\end{equation}\]</div>
<p>This implies that the posterior will be a truncated Gaussian, and its maximum will always be above zero.</p>
<p>This also implies that we cannot use the Gaussian approximation. Instead we will do the proper calculation using the transformation <a class="reference internal" href="#equation-eq-bayesianadvantage-transformation">(7.15)</a></p>
<div class="amsmath math notranslate nohighlight" id="equation-a005867b-1fcc-4d88-92d6-d395a6f288d6">
<span class="eqno">(7.28)<a class="headerlink" href="#equation-a005867b-1fcc-4d88-92d6-d395a6f288d6" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(f|{\data},I) = \pdf{A}{{\data},I} \left| \frac{dA}{df} \right| = 2 f \pdf{A}{{\data},I}
\end{equation}\]</div>
<p>In the end we find the proper Bayesian error propagation given by the PDF</p>
<div class="amsmath math notranslate nohighlight" id="equation-ad106fed-88dc-4e1c-bcf3-cbdd53e48130">
<span class="eqno">(7.29)<a class="headerlink" href="#equation-ad106fed-88dc-4e1c-bcf3-cbdd53e48130" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(f|{\data},I) \propto \left\{
\begin{array}{ll}
f \exp \left[ -\frac{(A-A_0)^2}{2\sigma_A^2} \right], &amp; 0 \le f \le \sqrt{A_\mathrm{max}}, \\
0, &amp; \mathrm{otherwise}.
\end{array}
\right.
\end{equation}\]</div>
<p><a class="reference internal" href="#fig-example-bayesianadvantage-taking-square-root"><span class="std std-numref">Fig. 7.3</span></a> visualize the difference between the Bayesian and the naive error propagation for a few scenarios. The code to generate these plots is in the hidden cell below.</p>
</section>
</div><figure class="align-default" id="fig-example-bayesianadvantage-taking-square-root">
<img alt="../../../_images/786130fb7e05b544b77e9132f102f23819383598df7f364d922a7f027f8dba09.png" src="../../../_images/786130fb7e05b544b77e9132f102f23819383598df7f364d922a7f027f8dba09.png" />
<figcaption>
<p><span class="caption-number">Fig. 7.3 </span><span class="caption-text">The left-hand panels show the posterior PDF for the amplitude of a Bragg peak in three different scenarios. The right-hand plots are the corresponding PDFs for the modulus of the structure factor <span class="math notranslate nohighlight">\(f=\sqrt{A}\)</span>. The solid lines correspond to a full Bayesian error propagation, while the dashed lines are obtained with the short-cut error propagation.</span><a class="headerlink" href="#fig-example-bayesianadvantage-taking-square-root" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">myst_nb</span><span class="w"> </span><span class="kn">import</span> <span class="n">glue</span>

<span class="k">def</span><span class="w"> </span><span class="nf">A_posterior</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">pA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">A0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigA</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pA</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pA</span><span class="p">)</span>

<span class="c1"># Wrong analysis</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f_likelihood</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">sigf</span> <span class="o">=</span> <span class="n">sigA</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A0</span><span class="p">))</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A0</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigf</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pf</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span>

<span class="c1"># Correct error propagation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f_posterior</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">):</span>
    <span class="n">pf</span> <span class="o">=</span> <span class="n">f</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">A0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigA</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pf</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span>
    
<span class="n">fig_Af</span><span class="p">,</span><span class="n">axs</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="k">for</span> <span class="n">iA</span><span class="p">,</span> <span class="p">(</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([(</span><span class="mi">9</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">),(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span><span class="mi">9</span><span class="p">)]):</span>
    <span class="n">maxA</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">A0</span><span class="p">,</span><span class="mi">3</span><span class="o">*</span><span class="n">sigA</span><span class="p">)</span>
    <span class="n">A_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="n">maxA</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">f_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">A_arr</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">A_arr</span><span class="p">,</span><span class="n">A_posterior</span><span class="p">(</span><span class="n">A_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">f_posterior</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Bayesian&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">A0</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">f_likelihood</span><span class="p">(</span><span class="n">f_arr</span><span class="p">,</span><span class="n">A0</span><span class="p">,</span><span class="n">sigA</span><span class="p">),</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Naive&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p(A | \mathcal</span><span class="si">{D}</span><span class="s1">,I)$&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;$A_0=</span><span class="si">{</span><span class="n">A0</span><span class="si">}</span><span class="s1">$, $\sigma_A=</span><span class="si">{</span><span class="n">sigA</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">,</span> \
    	<span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span>\
    	<span class="n">transform</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">iA</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$p(f | \mathcal</span><span class="si">{D}</span><span class="s1">,I)$&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">fig_Af</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Af_fig&quot;</span><span class="p">,</span> <span class="n">fig_Af</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/786130fb7e05b544b77e9132f102f23819383598df7f364d922a7f027f8dba09.png" src="../../../_images/786130fb7e05b544b77e9132f102f23819383598df7f364d922a7f027f8dba09.png" />
</div>
</details>
</div>
</section>
<section id="solutions">
<h2>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h2>
<div class="solution dropdown admonition" id="solution:BayesianAdvantage:correlated-errors">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantage:correlated-errors"> Exercise 7.1 (Correlated errors)</a></p>
<section id="solution-content">
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\pdf{z}{I} &amp;= \int \pdf{z}{x,y,I} \pdf{x,y}{I} dxdy = \int \delta(z-\sqrt{2}s) p(s|I) p(t|I) dsdt
\\
&amp;= \left\{ 
\begin{array}{l}
\tilde{s}=\sqrt{2}s \, \Rightarrow \, ds = d\tilde{s}/\sqrt{2} \\
\delta(z-\sqrt{2}s) \to \delta(z/\sqrt{2}-\tilde{s})
\end{array}
\right\}
= \frac{1}{\sqrt{2}\sqrt{2\pi}\sigma_s}\exp \left( - \frac{z^2}{4\sigma_s^2} \right) \int p(t|I) dt .
\end{align*}\]</div>
<p>Here we can use that <span class="math notranslate nohighlight">\(\int p(t|I) dt = 1\)</span>. Defining <span class="math notranslate nohighlight">\(\sigma_z = \sqrt{2}\sigma_s\)</span> we find</p>
<div class="math notranslate nohighlight">
\[
\pdf{z}{I} = \frac{1}{\sqrt{2\pi}\sigma_z}\exp \left( - \frac{z^2}{2\sigma_z^2} \right) ,
\]</div>
<p>i.e. a normal distribution for <span class="math notranslate nohighlight">\(z\)</span> with a small variance <span class="math notranslate nohighlight">\(2\sigma_s^2\)</span>.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:BayesianAdvantages:inferring-galactic-distances-ex">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantages:inferring-galactic-distances-ex"> Exercise 7.3 (Inferring galactic distances)</a></p>
<section id="solution-content">
<ol class="arabic">
<li><p>A fixed value for <span class="math notranslate nohighlight">\(H\)</span> can be assigned with the PDF <span class="math notranslate nohighlight">\(\pdf{H}{I} = \delta(H-H_0)\)</span>, where <span class="math notranslate nohighlight">\(\delta(x)\)</span> is the Kronecker delta. We note that integrals over a delta function are given by <span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty} f(x) \delta(x-x_0) dx = f(x_0)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
   \pdf{d}{\data,I} \propto \exp\left( - \frac{(v_0 - H_0 d)^2}{2\sigma_v^2} \right),
   \]</div>
<p>where we have ignored all normalization coefficients.</p>
</li>
<li><p>With a Gaussian prior for <span class="math notranslate nohighlight">\(H\)</span> we will be left with an integral</p>
<div class="math notranslate nohighlight">
\[
   \pdf{d}{\data,I} \propto \int_{-\infty}^{+\infty} dH \exp\left( - \frac{(H-H_0)^2}{2  \sigma_H^2} \right) \exp\left( - \frac{(v_0 - H d)^2}{2\sigma_v^2} \right).
   \]</div>
<p>We can perform this integral numerically, or we can solve it analytically by realizing that the product of two Gaussian distributions is another Gaussian distribution.</p>
</li>
<li><p>The uniform prior for <span class="math notranslate nohighlight">\(\pdf{H}{I}\)</span> implies that the final integral becomes</p>
<div class="math notranslate nohighlight">
\[
   \pdf{d}{\data,I} \propto \int_{H_0 - 2\sigma_H}^{H_0 + 2\sigma_H} dH \exp\left( - \frac{(v_0 - H d)^2}{2\sigma_v^2} \right).
   \]</div>
</li>
</ol>
<p>For numerical integration, and plots of the results of the three inference strategies, see the hidden code block below.</p>
</section>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Common imports</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">integrate</span>

<span class="c1"># Given information</span>
<span class="n">H0</span><span class="o">=</span><span class="mi">70</span>
<span class="n">sigH</span><span class="o">=</span><span class="mi">10</span>
<span class="n">v0</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span>
<span class="n">sigv</span><span class="o">=</span><span class="mi">5000</span>

<span class="c1"># grid for evaluating and plotting p(d|...)</span>
<span class="n">dgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span><span class="mi">2500</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Case 1</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pdf_1</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">v0</span><span class="o">-</span><span class="n">H0</span><span class="o">*</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigv</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># Case 2</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pdf_2</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">H</span><span class="o">-</span><span class="n">H0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigH</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">v0</span><span class="o">-</span><span class="n">H</span><span class="o">*</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigv</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># ... with numerical integration over H</span>
<span class="n">pdf_2_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dgrid</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i_d</span><span class="p">,</span><span class="n">di</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dgrid</span><span class="p">):</span>
	<span class="n">I</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="n">pdf_2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">di</span><span class="p">))</span>
	<span class="n">pdf_2_grid</span><span class="p">[</span><span class="n">i_d</span><span class="p">]</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">20</span>
<span class="c1"># Case 3</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pdf_3</span><span class="p">(</span><span class="n">H</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">v0</span><span class="o">-</span><span class="n">H</span><span class="o">*</span><span class="n">d</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigv</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># ... with numerical integration</span>
<span class="n">pdf_3_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">dgrid</span><span class="p">)</span>
<span class="n">deltaH</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">12</span><span class="o">*</span><span class="n">sigH</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i_d</span><span class="p">,</span><span class="n">di</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dgrid</span><span class="p">):</span>
	<span class="n">I</span> <span class="o">=</span> <span class="n">integrate</span><span class="o">.</span><span class="n">quad</span><span class="p">(</span><span class="n">pdf_3</span><span class="p">,</span> <span class="n">H0</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">sigH</span><span class="p">,</span> <span class="n">H0</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">sigH</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">di</span><span class="p">))</span>
	<span class="n">pdf_3_grid</span><span class="p">[</span><span class="n">i_d</span><span class="p">]</span> <span class="o">=</span> <span class="n">I</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">30</span>
<span class="c1"># Case 4 (see example 9.3)</span>
<span class="n">d0</span> <span class="o">=</span> <span class="n">v0</span><span class="o">/</span><span class="n">H0</span>
<span class="n">sigd</span> <span class="o">=</span> <span class="n">d0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigH</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">H0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">sigv</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">v0</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pdf_4</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="n">d0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigd</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
    
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dgrid</span><span class="p">,</span><span class="n">pdf_1</span><span class="p">(</span><span class="n">dgrid</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Case 1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dgrid</span><span class="p">,</span><span class="n">pdf_2_grid</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Case 2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dgrid</span><span class="p">,</span><span class="n">pdf_3_grid</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Case 3&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dgrid</span><span class="p">,</span><span class="n">pdf_4</span><span class="p">(</span><span class="n">dgrid</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Case 4&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$d$ [Mpc]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(d|\mathcal</span><span class="si">{D}</span><span class="s1">,I)$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/8aeecc274e24287f8ed021f21dd41f988339cdda8a51a91645807faff6c09fe0.png" src="../../../_images/8aeecc274e24287f8ed021f21dd41f988339cdda8a51a91645807faff6c09fe0.png" />
</div>
</details>
</div>
<div class="solution dropdown admonition" id="solution:BayesianAdvantages:standard-random-variable">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantages:standard-random-variable"> Exercise 7.4 (The standard random variable)</a></p>
<section id="solution-content">
<p>The transformation <span class="math notranslate nohighlight">\(z = f(x) = (x-\mu)/\sigma\)</span> gives the inverse <span class="math notranslate nohighlight">\(x = f^{-1}(z) = \sigma z + \mu\)</span> and the Jacobian <span class="math notranslate nohighlight">\(|dx/dz = \sigma|\)</span>.</p>
<p>Therefore <span class="math notranslate nohighlight">\(\pdf{z}{I} = \pdf{x}{I} \sigma\)</span>. With the given form of <span class="math notranslate nohighlight">\(\pdf{x}{I}\)</span> we get</p>
<div class="math notranslate nohighlight">
\[
\pdf{z}{I} = \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{z^2}{2}\right),
\]</div>
<p>which corresponds to a Gaussian distribution with mean zero and variance one, sometimes known as a standard random variable.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:BayesianAdvantages:square-root-of-a-number">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantages:square-root-of-a-number"> Exercise 7.5 (The square root of a number)</a></p>
<section id="solution-content">
<p>With <span class="math notranslate nohighlight">\(z = f(x) = \sqrt{x}\)</span> we have <span class="math notranslate nohighlight">\(x = f^{-1}(z) = z^2\)</span> such that <span class="math notranslate nohighlight">\(|dx/dz| = 2|z|\)</span>. We note that <span class="math notranslate nohighlight">\(z\)</span> is positive such that <span class="math notranslate nohighlight">\(|z| = z\)</span> and we therefore have</p>
<div class="math notranslate nohighlight">
\[
\pdf{z}{I} = 2 z \pdf{x}{I} = 2 z \frac{1}{x_{\max} - x_{\min}} \quad \text{for } \sqrt{x_{\min}} \leq z \leq \sqrt{x_{\max}},
\]</div>
<p>and 0 elsewhere.</p>
<p>We check the normalization by performing the integral</p>
<div class="math notranslate nohighlight">
\[
\int_0^\infty \pdf{z}{I} dz = \int_{\sqrt{x_{\min}}}^{\sqrt{x_{\max}}} \frac{2z}{x_{\max} - x_{\min}} dz = \frac{1}{x_{\max} - x_{\min}} \left[ z^2 \right]_{\sqrt{x_{\min}}}^{\sqrt{x_{\max}}} = 1.
\]</div>
</section>
</div>
<div class="solution dropdown admonition" id="solution:BayesianAdvantages:gaussian-sum-of-errors">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantages:gaussian-sum-of-errors"> Exercise 7.6 (Gaussian sum of errors)</a></p>
<section id="solution-content">
<p>The PDF <span class="math notranslate nohighlight">\(\pdf{Z}{I}\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\expect{Z} = z_0 = x_0+y_0\)</span> and variance <span class="math notranslate nohighlight">\(\var{Z} = \sigma_z^2 = \sigma_x^2 + \sigma_y^2\)</span>, where <span class="math notranslate nohighlight">\(x_0,y_0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x^2, \sigma_y^2\)</span> are the means and variances of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, respectively.</p>
<p>This is the same result as in <a class="reference internal" href="#example:BayesianAdvantage:Z=X+Y">Example 7.1</a> which should not be surprising since the errors were in fact Gaussian.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:BayesianAdvantages:gaussian-product-of-errors">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:BayesianAdvantages:gaussian-product-of-errors"> Exercise 7.7 (Gaussian product of errors)</a></p>
<section id="solution-content">
<p>The PDF <span class="math notranslate nohighlight">\(\pdf{Z}{I}\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\expect{Z} = z_0 = x_0 y_0\)</span> and variance <span class="math notranslate nohighlight">\(\var{Z} = \sigma_z^2 = y_0^2 \sigma_x^2 + x_0^2 \sigma_y^2\)</span>, where <span class="math notranslate nohighlight">\(x_0,y_0\)</span> and <span class="math notranslate nohighlight">\(\sigma_x^2, \sigma_y^2\)</span> are the means and variances of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, respectively.</p>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/BayesianStatistics/BayesianBasics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="UsingBayes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Bayes in practice</p>
      </div>
    </a>
    <a class="right-next"
       href="../BayesianWorkflow/BayesianWorkflow.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.2. </span>Bayesian research workflow</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-with-parametric-models">Inference with parametric models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-i-nuisance-parameters-and-marginalization">Error propagation (I): Nuisance parameters and marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-ii-changing-variables">Error propagation (II): Changing variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-propagation-iii-a-useful-approximation">Error propagation (III): A useful approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian Forssén, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>