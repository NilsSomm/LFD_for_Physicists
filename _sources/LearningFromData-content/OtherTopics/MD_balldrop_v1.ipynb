{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model discrepancy example: The ball-drop experiment\n",
    "\n",
    "A ball is dropped from a tower of height $h_0$ at initial time $t_0=0$ with intial velocity $v_0=0$. The height from the ground is recorded at discrete time points. Data is generated based on a model with drag, and includes uncorrelated Gaussian measurement errors. The aim is to extract the value of accelaration due to gravity, $g$.\n",
    "\n",
    "The problem is that the data is noisy *and* our theoretical model, which is simply free fall without drag, is incorrect. How can we best formulate our analysis such that the extraction of $g$ is robust? I.e., that we get a distribution for $g$ that covers the true answer with plausible uncertainty.\n",
    "\n",
    "**This notebook was created by Dr. Sunil Jaiswal in July, 2025.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical formulation\n",
    "\n",
    "We will follow the article by [Higdon et.al.](https://epubs.siam.org/doi/10.1137/S1064827503426693)\n",
    "The statistical equation accounting for model discrepancy is:\n",
    "\n",
    "\\begin{equation*}\n",
    "    y(x_i) = \\eta(x_i; {\\boldsymbol \\theta}) + \\delta(x_i;{\\boldsymbol \\phi}) + \\epsilon(x_i;{\\boldsymbol \\sigma_{\\boldsymbol \\epsilon}}) \\,, \\quad i =1, \\dots , n\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "where $x$ is the input vector (time in the case of the ball drop experiment) of length $n$. \n",
    "* We shall denote the input domain as $\\{x_1,\\cdots,x_n\\}\\in \\chi_*$.\n",
    "* The above equation should be understood as an equation which holds on the range (set of all possible outcomes) of the probability distributions. It means that a sample drawn at a given $x_i$ from the probability distributions $\\mathrm{pr}(y)$ should be equal to the sum of the sample drawn from each of the distributions $\\mathrm{pr}(\\eta|{\\boldsymbol \\theta})$, $\\mathrm{pr}(\\delta|{\\boldsymbol \\phi})$ and $\\mathrm{pr}(\\epsilon|{\\boldsymbol \\sigma_{\\boldsymbol \\epsilon}})$.\n",
    "* Note that for given $x_i$, $\\mathrm{pr}(\\eta(x_i)|{\\boldsymbol \\theta}) = \\delta(\\eta(x_i)-\\eta(x_i;{\\boldsymbol \\theta}))$ is deterministic since the model is deterministic given ${\\boldsymbol \\theta}$ (note: $\\delta$ is the Dirac delta function).\n",
    "\n",
    "\n",
    "In Eq. (1), $y(x_i)$ are vectors of the measured observables (velocity and height) at each input point $x_i$. \n",
    "* The error in the measurement of $y(x_i)$ is represented by $\\epsilon(x_i;{\\boldsymbol \\sigma_{\\boldsymbol \\epsilon}})$, which is assumed to be independently and identically distributed $\\sim N(0, {\\boldsymbol \\sigma_{\\boldsymbol \\epsilon}}^2)$ for each type of observable (velocity and height: $\\{\\sigma_v, \\sigma_h\\} \\in {\\boldsymbol \\sigma_{\\boldsymbol \\epsilon}}$).\n",
    "* Note that $\\{\\sigma_v, \\sigma_h\\}$ are fixed constants since we have assumed the distribution for each type of observable to be iid's. We shall treat them as information `$I$' whenever we write any probability distribution $\\mathrm{pr}(\\bullet|\\bullet,I)$, and will not state them explicitly.\n",
    "* $\\eta(x;{\\boldsymbol \\theta})$ denotes the observable predictions of the physics theory given the value of the parameters of the theory $\\{g, \\sigma_{v_0}\\}\\in {\\boldsymbol \\theta}$.\n",
    "* The stochastic model $\\mathrm{pr}(\\delta(x)|{\\boldsymbol \\phi}) = GP[0,\\kappa(x, x';{\\boldsymbol \\phi})]$, accounts for model discrepancy (discrepancy between the physics model and reality) where $\\kappa$ is covariance kernel of the Gaussian Process (GP) with  $\\{\\bar{c}_v,\\, l_v,\\, \\bar{c}_h,\\, l_h\\}\\in {\\boldsymbol \\phi}$ representing the hyperparameters.\n",
    "* We define   $\\{{\\boldsymbol \\theta}, {\\boldsymbol \\phi}\\} \\in {\\boldsymbol \\alpha}$ as a vector containing ${\\boldsymbol \\theta}$ and ${\\boldsymbol \\phi}$ for compact notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Consider the drag force on the ball (sphere) for the true model to behave as\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\bf f}_D = -(b v + c v^2)\\, \\hat{\\bf{v}} \\,, \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "<!-- $${\\bf f}_D = -(b v + c v^2)\\, \\hat{\\bf{v}} \\,, \\tag(2)$$ -->\n",
    "where $v$ is magnitude of velocity $\\bf{v}$.\n",
    "The coefficients $b$ and $c$ are drag coefficients.\n",
    "\n",
    "$\\bullet$ Equation of motion: \n",
    "\n",
    "$$m\\frac{d\\bf{v}}{dt} = m {\\bf g} - b {\\bf v}- c v^2 \\hat{\\bf{v}} \\,.$$ \n",
    "\n",
    "We have considerd the downward direction to be positive. Here $\\color{red}{m=1\\, kg}$ is the mass of the sphere and $\\color{red}{g=9.8\\, m/s^2}$ the acceleration due to gravity. \n",
    "\n",
    "\n",
    "$\\bullet$ The equation for velocity can be written as $m\\frac{d v}{dt} = m g - b v- c v \\sqrt{v^2}$. The last term is written such that it takes into account the direction of velocity in quadratic term. We need to solve the equations:\n",
    "\n",
    "$$\\frac{d v}{dt} = g - \\frac{b}{m} v- \\frac{c}{m} v |v| \\,, \\qquad \n",
    "\\frac{dh}{dt} = -v \\,.$$\n",
    "\n",
    "By default we consider the ball to have the initial height $\\color{red}{h_0=60\\, m}$ at initial time $\\color{red}{t_0=0\\,sec}$ with initial velocity $\\color{red}{v_0=0\\, m/s^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set plot style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the local CodeLocal directory to the path so Jupyter finds the module file.\n",
    "import sys\n",
    "sys.path.append('./CodeLocal')\n",
    "from setup_rc_params import setup_rc_params  # import from setup_rc_params.py in subdirectory CodeLocal\n",
    "\n",
    "setup_rc_params(presentation=True, constrained_layout=True, usetex=True, dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup sampling with emcee and pocomc samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pocomc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01memcee\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpocomc\u001b[39;00m   \u001b[38;5;66;03m# uncomment here when using pocomc sampler\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pool  \n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uniform\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pocomc'"
     ]
    }
   ],
   "source": [
    "import emcee\n",
    "import pocomc   # uncomment here when using pocomc sampler\n",
    "from multiprocess import Pool  \n",
    "from scipy.stats import uniform\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ================================ emcee sampler ==================================\n",
    "def emcee_sampling(min_param, max_param, log_posterior, \n",
    "                   nburn=2000, nsteps=2000, num_walker_per_dim=8, progress=False):\n",
    "    \"\"\"\n",
    "    Function for sampling in parallel using emcee.\n",
    "    \n",
    "    Parameters:\n",
    "      - min_param (list or array): minimum values parameters\n",
    "      - max_param (list or array): maximum values parameters\n",
    "      - log_posterior (callable): log posterior\n",
    "      - nburn (int): \"burn-in\" period to let chains stabilize\n",
    "      - nsteps (int): # number of MCMC steps to take after burn-in\n",
    "      - num_walker_per_dim (int): number of walker per dimension\n",
    "      - samples_save_dir (string): directory name in which the samples will be saved\n",
    "      - progress (boolean): toggle the progress bar\n",
    "    \"\"\"\n",
    "    if len(min_param) != len(max_param):\n",
    "            raise ValueError(\n",
    "                f\"min_param and max_param must have the same length.\"\n",
    "            )\n",
    "\n",
    "    ndim = len(min_param)  # number of parameters in the model\n",
    "    nwalkers = ndim * num_walker_per_dim  # number of MCMC walkers\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Starting MCMC sampling using emcee with {nwalkers} walkers in parallel...\")\n",
    "\n",
    "    # Generate starting guesses within the prior bounds\n",
    "    starting_guesses =  min_param + (max_param - min_param) * np.random.rand(nwalkers,ndim)\n",
    "    \n",
    "    # Create a pool of workers using multiprocessing\n",
    "    with Pool() as pool:\n",
    "        # Pass the pool to the sampler\n",
    "        sampler = emcee.EnsembleSampler(nwalkers,ndim,log_posterior,pool=pool)\n",
    "    \n",
    "        # \"Burn-in\" period\n",
    "        pos, prob, state = sampler.run_mcmc(starting_guesses, nburn, progress=progress)\n",
    "        sampler.reset()\n",
    "    \n",
    "        # Sampling period\n",
    "        pos, prob, state = sampler.run_mcmc(pos, nsteps, progress=progress)\n",
    "\n",
    "    # Collect samples\n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Sampling complete. Time taken: {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    print(f\"Mean acceptance fraction: {np.mean(sampler.acceptance_fraction):.3f} (in total {nwalkers*nsteps} steps)\")\n",
    "    print(f\"Samples shape: {samples.shape}\")\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# ================================ pocoMC sampler ==================================\n",
    "def pocomc_sampling(min_param, max_param, log_posterior,\n",
    "                    n_effective=2000, n_active=800, n_steps=None,\n",
    "                    # n_prior=2048, n_max_steps=200, \n",
    "                    n_total=5000, n_evidence=5000, ncores = -1\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    This function is based on PocoMC package (version 1.2.1).\n",
    "    pocoMC is a Preconditioned Monte Carlo (PMC) sampler that uses \n",
    "    normalizing flows to precondition the target distribution.\n",
    "\n",
    "    Parameters:\n",
    "      - min_param (list or array): Minimum values parameters\n",
    "      - max_param (list or array): Maximum values parameters\n",
    "      - log_posterior (callable): Log posterior\n",
    "\n",
    "      - n_effective (int): The effective sample size maintained during the run. Default: 2000.\n",
    "      - n_active (int): Number of active particles. Default: 800. Must be < n_effective.\n",
    "      - n_steps (int): Number of MCMC steps after logP plateau. Default: None -> n_steps=n_dim. \n",
    "                       Higher values lead to better exploration but increases computational cost.\n",
    "      - n_total (int): Total effectively independent samples to be collected. Default: 5000.\n",
    "      - n_evidence (int): Number of importance samples used to estimate evidence. Default: 5000. \n",
    "                          If 0, the evidence is not estimated using importance sampling.\n",
    "\n",
    "      - ncores: Number of cores to use in sampling. \n",
    "                  -1 (default): use all availaible cores\n",
    "                   n (int): use n cores\n",
    "    \"\"\"\n",
    "\n",
    "    # Generating prior range for pocomc\n",
    "    priors = [uniform(lower, upper - lower) for lower, upper in zip(min_param, max_param)]\n",
    "    prior = pocomc.Prior(priors)\n",
    "    \n",
    "    ndim = len(min_param)  # number of parameters in the model\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------- \n",
    "    # Try to get the allowed CPU count via affinity (works on Linux/Windows)\n",
    "    try:\n",
    "        available_cores = len(os.sched_getaffinity(0))# len(psutil.Process().cpu_affinity())\n",
    "    except Exception:\n",
    "        # On systems where affinity is not available (e.g., macOS), use total CPU count\n",
    "        available_cores = os.cpu_count()\n",
    "\n",
    "    # Validate that ncores is an integer and at least -1\n",
    "    if not isinstance(ncores, int) or ncores < -1:\n",
    "        raise ValueError(\"ncores must be an integer greater than or equal to -1. Specify '-1' to use all available cores.\")\n",
    "    \n",
    "    if ncores == -1:\n",
    "        # Use all available cores\n",
    "        num_cores = available_cores\n",
    "    elif ncores > available_cores:\n",
    "        # Warn if more cores are requested than available and then use available cores\n",
    "        warnings.warn(\n",
    "            f\"ncores ({ncores}) exceeds available cores ({available_cores}). Using available cores.\",\n",
    "            UserWarning\n",
    "        )\n",
    "        num_cores = available_cores\n",
    "    else:\n",
    "        num_cores = ncores     \n",
    "    # --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    sampler = pocomc.Sampler(prior=prior, likelihood=log_posterior,\n",
    "                                 n_effective=n_effective, n_active=n_active, n_steps=n_steps,\n",
    "                                 # dynamic=True, n_prior=n_prior, n_max_steps=n_max_steps,\n",
    "                                 pytorch_threads=None,  # use all availaible threads for training\n",
    "                                 pool=num_cores\n",
    "                                 )\n",
    "\n",
    "    print(f\"Starting sampling using pocomc in {num_cores} cores...\")\n",
    "\n",
    "    sampler.run(n_total=n_total, \n",
    "                n_evidence=n_evidence, \n",
    "                progress=True)\n",
    "\n",
    "\n",
    "    # Generating the posterior samples\n",
    "    # samples, weights, logl, logp = sampler.posterior() # Weighted posterior samples\n",
    "    samples, _, _ = sampler.posterior(resample=True)  # equal weights for samples \n",
    "    \n",
    "    # Generating the evidence\n",
    "    # logz, logz_err = sampler.evidence() # Bayesian model evidence estimate and uncertainty\n",
    "    # print(\"Log evidence: \", logz)\n",
    "    # print(\"Log evidence error: \", logz_err)\n",
    "    \n",
    "    # logging.info('Writing pocoMC chains to file...')\n",
    "    # chain_data = {'chain': samples, 'weights': weights, 'logl': logl,\n",
    "    #                 'logp': logp, 'logz': logz, 'logz_err': logz_err}\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Sampling complete. Time taken: {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    print(f\"Samples shape: {samples.shape}\")\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate \"experimental\" observations for height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "def True_height(t, h0=60.0, v0=0.0,\n",
    "                g=9.8, m=1.0, b=0.0, c=0.4):    # initial conditions\n",
    "    \"\"\"\n",
    "    Height of a falling ball with linear + quadratic air drag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float or 1-D array_like\n",
    "        Time(s) at which to evaluate the height.\n",
    "    h0, v0 : float, optional\n",
    "        Initial height and velocity at time t = 0.\n",
    "    g : float, optional\n",
    "        Acceleration due to gravity (m s⁻²) (taken positive downward).\n",
    "    m : float, optional\n",
    "        Mass of the ball (kg).\n",
    "    b, c : float, optional\n",
    "        Linear and quadratic drag coefficients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h : 1darray\n",
    "        Height(s) at the requested time points (same shape as `t`).\n",
    "    \"\"\"\n",
    "\n",
    "    def derivatives(y, _t):\n",
    "        h, v = y\n",
    "        dhdt = -v\n",
    "        dvdt = g - (b*v)/m - (c*abs(v)*v)/m\n",
    "        return [dhdt, dvdt]\n",
    "\n",
    "    # Ensure we integrate from t0 out to the requested time(s)\n",
    "    t = np.atleast_1d(t)\n",
    "    times = np.concatenate(([t[0]], t))  # add t[0] to get solution at t=0 \n",
    "    sol = odeint(derivatives, [h0, v0], times)\n",
    "    \n",
    "    return sol[1:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Set a random seed for reproducibility. Change 42 -> 0 for different seeds every time.\n",
    "\n",
    "# Generate an array of time where height are observed -->\n",
    "t_obs = np.linspace(0.0, 1.0, 10)\n",
    "t_noise = np.random.uniform(1e-3, 1e-2, size = t_obs.shape)  # random noise added to time. helps in inversion of covariance matrix.\n",
    "t_noise[0] = 0.0  # No noise for initial time\n",
    "t_obs = np.round(t_obs + t_noise, decimals=4)\n",
    "\n",
    "\n",
    "# Compute height of ball at these time steps -->\n",
    "hexp_std = 0.1 # standard deviation of assumed gaussian noise as error of measurment\n",
    "hexp_err = np.array([hexp_std]*len(t_obs))  # adding gaussian noise to observation of height\n",
    "hexp_mean = np.random.normal(True_height(t_obs), hexp_err)    # observed height at input times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3)) \n",
    "plt.errorbar(t_obs, hexp_mean, yerr=[1.96*hexp_err, 1.96*hexp_err], \n",
    "             fmt='o', markersize=3, capsize=3, color='black', \n",
    "             label=f'``Experiment\": 95\\% CI')\n",
    "plt.ylabel('height')\n",
    "plt.xlabel('time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define physics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_height(t, g, v0=0.0, h0=60.0):\n",
    "    return np.atleast_1d(h0  - v0 * t - g * t**2 / 2.0 )\n",
    "\n",
    "# def model_height(t, g, v0, h0=60.0):\n",
    "#     f = True_height(t, h0=60.0, v0=v0, g=g, m=1.0, b=0.0, c=0.0)\n",
    "#     return np.atleast_1d(f)\n",
    "# model_height(t=t_obs, g = 9.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model discrepancy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "class ModelDiscrepancy():\n",
    "    def __init__(self, t_obs, hexp_mean, hexp_err, model_height, num_model_param, log_priors, MD = False, kernel_func = None):\n",
    "\n",
    "        self.t_obs = t_obs\n",
    "        self.hexp_mean = hexp_mean\n",
    "        self.hexp_err = hexp_err\n",
    "        self.model_height = model_height\n",
    "        self.num_model_param = num_model_param\n",
    "        self.log_priors = log_priors\n",
    "        self.MD = MD\n",
    "        self.kernel_func = kernel_func\n",
    "\n",
    "        if self.MD:\n",
    "            self.log_priors_model = self.log_priors[:self.num_model_param]\n",
    "            self.log_priors_MDGP = self.log_priors[self.num_model_param:]\n",
    "        else:\n",
    "            self.log_priors_model = self.log_priors\n",
    "        \n",
    "        # standardize experimental data  ----->\n",
    "        self.scaler = StandardScaler()\n",
    "        self.hexp_mean_scaled = self.scaler.fit_transform(self.hexp_mean.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        self.offset = float(self.scaler.mean_[0])\n",
    "        self.scale = float(self.scaler.scale_[0])\n",
    "        \n",
    "        self.hexp_err_scaled = self.hexp_err / self.scale\n",
    "    #=======================================================================\n",
    "    \n",
    "    def log_likelihood(self, theta, phi=[]):\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of a dataset given the mean and covariance matrix.\n",
    "    \n",
    "        Parameters:\n",
    "            theta: 1D NumPy array containing model parameters.\n",
    "            phi: 1D NumPy array containing GP kernel hyperparameters.\n",
    "    \n",
    "        Returns:\n",
    "            Log-likelihood of the dataset.\n",
    "        \"\"\"\n",
    "    \n",
    "        if self.num_model_param == 1:\n",
    "            g = theta[0]\n",
    "            h = model_height(t = self.t_obs, g = g)\n",
    "        elif self.num_model_param == 2:\n",
    "            g, v0 = theta[0], theta[1]\n",
    "            h = self.model_height(t = self.t_obs, g = g, v0 = v0)\n",
    "    \n",
    "        hmodel_scaled = (h - self.offset) / self.scale\n",
    "    \n",
    "        cov_exp = np.diag(self.hexp_err_scaled**2)\n",
    "        \n",
    "        if self.MD == False:\n",
    "            cov_total = cov_exp\n",
    "        else:\n",
    "            X = self.t_obs.reshape(-1, 1)\n",
    "            cov_total = self.kernel_func(X, X, *phi) + cov_exp\n",
    "                                \n",
    "        k = len(self.t_obs)  # number of observables\n",
    "    \n",
    "        md2 = mahalanobis(self.hexp_mean_scaled, hmodel_scaled, np.linalg.inv(cov_total))**2\n",
    "    \n",
    "        # calculate the log-likelihood\n",
    "        log_likelihood_value = -0.5 * (md2 + np.log(np.linalg.det(cov_total)) + k * np.log(2.0 * np.pi) )\n",
    "                                    \n",
    "        return log_likelihood_value\n",
    "    #=======================================================================\n",
    "    \n",
    "    def log_posterior(self, theta_and_phi):\n",
    "        \"\"\"\n",
    "        Calculate the log posterior = sum of log priors + log likelihood.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta_and_phi : np.ndarray (1D)\n",
    "            Combined array of model params (theta) and discrepancy params (phi) if MD=True.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The log posterior value.\n",
    "        \"\"\"\n",
    "        theta_and_phi = np.atleast_1d(theta_and_phi)\n",
    "        \n",
    "        if self.MD:\n",
    "            theta = theta_and_phi[:self.num_model_param]\n",
    "            phi = theta_and_phi[self.num_model_param:]\n",
    "\n",
    "        else:\n",
    "            theta = theta_and_phi\n",
    "            phi = []\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # Calculate log-prior for model parameters\n",
    "        # ----------------------------------------\n",
    "        log_prior_val = 0.0\n",
    "        for i, prior_func in enumerate(self.log_priors_model):\n",
    "            lp = prior_func(theta[i])\n",
    "            if lp <= -1e5:\n",
    "                return -1e30\n",
    "            log_prior_val += lp\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # If MD ==True, accumulate log-prior for phi as well\n",
    "        # ------------------------------------------------------\n",
    "        if self.MD:\n",
    "            for i, prior_func in enumerate(self.log_priors_MDGP):\n",
    "                lp = prior_func(phi[i])\n",
    "                if lp <= -1e5:\n",
    "                    return -1e30\n",
    "                log_prior_val += lp\n",
    "\n",
    "        # ------------------------\n",
    "        # Calculate log-likelihood\n",
    "        # ------------------------\n",
    "        log_likelihood_val = self.log_likelihood(theta, phi)\n",
    "\n",
    "        return log_prior_val + log_likelihood_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "# Define function for general beta priors \n",
    "def log_prior_param(param, alpha_val, beta_val, param_min, param_max):\n",
    "    \"\"\"\n",
    "    Log prior for parameter based on a Beta distribution.\n",
    "    \"\"\"\n",
    "    scale = param_max - param_min  # Rescaling factor\n",
    "\n",
    "    if param_min < param < param_max:\n",
    "        # Rescale param to the range [param_min, param_max]\n",
    "        param_rescaled = (param - param_min) / scale\n",
    "        \n",
    "        prior = beta.pdf(param_rescaled, alpha_val, beta_val) / scale\n",
    "        return np.log(prior)\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_bounds = np.array([[ 0.0,  20.0 ],\n",
    "                               [-2.0 ,  2.0 ]])\n",
    "\n",
    "# model_param_bounds = np.array([[ 0.0,  20.0 ]])  # When only model parameter is g -- That's all the change required in code\n",
    "\n",
    "num_model_param = len(model_param_bounds)\n",
    "\n",
    "# Define a list of log_prior functions for model parameters\n",
    "log_prior_theta = [\n",
    "    lambda param, i=i: log_prior_param(\n",
    "        param, alpha_val=1.01, beta_val=1.01,  # for flat priors\n",
    "        param_min=model_param_bounds[i][0], param_max=model_param_bounds[i][1]\n",
    "    )\n",
    "    for i in range(num_model_param)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without model discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class  -->\n",
    "MDC = ModelDiscrepancy(t_obs=t_obs, hexp_mean=hexp_mean, hexp_err=hexp_err, \n",
    "                      model_height=model_height, num_model_param=num_model_param, \n",
    "                      log_priors=log_prior_theta, MD = False, kernel_func = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "guess = model_param_bounds.mean(axis=1)\n",
    "\n",
    "# Find Maximum likelihood value for parameters -->\n",
    "nll = lambda *args: - MDC.log_likelihood(*args)\n",
    "result = optimize.minimize(nll, guess,  method = 'Powell')\n",
    "max_lik = result['x']\n",
    "print(result)\n",
    "print(\"\\nMaximum likelihood value for parameters:\", max_lik,\"\\n\")\n",
    "\n",
    "\n",
    "# Find MAP values for parameters -->\n",
    "nlp = lambda *args: - MDC.log_posterior(*args)\n",
    "result = optimize.minimize(nlp, guess,  method = 'Powell')\n",
    "MAP = result['x']\n",
    "print(result)\n",
    "print(\"\\nMAP values for parameters:\", MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "min_param = model_param_bounds[:,0]\n",
    "max_param = model_param_bounds[:,1]\n",
    "lab = [r'$g$', r'$v_0$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do the sampling and plot the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_burn = 100  # 2000 is a minimum\n",
    "num_steps = 100  # 2000 is a minimum\n",
    "\n",
    "samples_WMD = emcee_sampling(min_param = min_param, max_param = max_param, \n",
    "                         log_posterior = MDC.log_posterior, \n",
    "                         nburn = num_burn, nsteps = num_steps, num_walker_per_dim = 8)\n",
    "\n",
    "fig = corner.corner(samples_WMD, labels=lab, color='C0', quantiles=[0.16, 0.5, 0.84],\n",
    "                   show_titles=True, title_kwargs={\"fontsize\": 12}, hist_kwargs={'density': True})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sampling with pocomc for consistency check ---> \n",
    "\n",
    "# n_effective_mltpl = 500\n",
    "# n_active_mltpl = 150  # 0.3 x n_eff\n",
    "# n_steps_mltpl = 2\n",
    "# num_param = len(min_param)\n",
    "\n",
    "# n_effective = n_effective_mltpl * num_param\n",
    "# n_active = n_active_mltpl * num_param\n",
    "# n_steps = n_steps_mltpl * num_param\n",
    "# # print(n_effective, n_active, n_steps)\n",
    "\n",
    "# n_total = 20000\n",
    "\n",
    "# samples_WMD_poco = pocomc_sampling(min_param, max_param, MDC.log_posterior,\n",
    "#                                   n_effective=n_effective, n_active=n_effective, n_steps=n_steps,\n",
    "#                                   n_total=n_total, n_evidence=n_total)\n",
    "\n",
    "# fig = corner.corner(samples_WMD_poco, labels=lab, color='C0', quantiles=[0.16, 0.5, 0.84],\n",
    "#                    show_titles=True, title_kwargs={\"fontsize\": 12}, hist_kwargs={'density': True})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With model discrepancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import DotProduct, RBF\n",
    "\n",
    "def MD_kernel(X1, X2, cbar, l, r, s):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix using a custom kernel:\n",
    "    K(X1, X2) = s^2 + cbar^2 * (X1 @ X2.T)^r * exp(-||X1 - X2||^2 / (2 * l^2)).\n",
    "\n",
    "    This kernel serves as the core of the Model Discrepancy framework and should be modified\n",
    "    to suit specific problems.\n",
    "\n",
    "    Parameters:\n",
    "        - X1 (numpy.ndarray): A matrix where each row represents an input vector.\n",
    "        - X2 (numpy.ndarray): A matrix where each row represents an input vector.\n",
    "        - cbar (float): The marginal variance parameter.\n",
    "        - l (float): The length scale parameter for the RBF term.\n",
    "        - r (float): The power applied to the dot product term.\n",
    "        - s (float): A constant shift.\n",
    "\n",
    "    Returns:\n",
    "        - numpy.ndarray: A covariance matrix computed using the defined kernel.\n",
    "    \"\"\"\n",
    "    # Compute the dot product term\n",
    "    dot_product_term = np.dot(X1, X2.T) ** r if r > 0 else 1.0\n",
    "\n",
    "    # Compute the RBF kernel term\n",
    "    rbf_kernel = RBF(length_scale=l)\n",
    "    rbf_term = rbf_kernel(X1, X2)\n",
    "\n",
    "    # Combine terms\n",
    "    cov_matrix =  s**2 + cbar**2 * dot_product_term * rbf_term\n",
    "    return cov_matrix\n",
    "    \n",
    "# ==========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter bounds and priors \n",
    "HP_bounds = np.array([[ 0.0,  10.0 ],     # for cbar\n",
    "                      [ 0.0 ,  10.0 ],    # for l\n",
    "                      [ 0.0 ,  10.0 ]     # for r\n",
    "                      # [ 0.0 ,  10.0 ]   # for s\n",
    "                     ])  \n",
    "\n",
    "# Define priors for GP hyperparmeters\n",
    "log_prior_cbar = lambda param: log_prior_param(param, alpha_val=1.01, beta_val=1.01, param_min=HP_bounds[0][0], param_max=HP_bounds[0][1])\n",
    "log_prior_l =    lambda param: log_prior_param(param, alpha_val=1.01, beta_val=1.01, param_min=HP_bounds[1][0], param_max=HP_bounds[1][1])\n",
    "log_prior_r =    lambda param: log_prior_param(param, alpha_val=1.01, beta_val=1.01, param_min=HP_bounds[2][0], param_max=HP_bounds[2][1])\n",
    "# log_prior_s =    lambda param: log_prior_param(param, alpha_val=1.01, beta_val=1.01, param_min=HP_bounds[3][0], param_max=HP_bounds[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = lambda X1, X2, cbar, l, r: MD_kernel(X1, X2, cbar, l, r, s=0)\n",
    "log_prior_phi = [log_prior_cbar, log_prior_l, log_prior_r]\n",
    "\n",
    "log_priors_thetaphi = np.concatenate([log_prior_theta, log_prior_phi])\n",
    "param_bounds = np.concatenate([model_param_bounds, HP_bounds])\n",
    "\n",
    "# Instantiate class  -->\n",
    "MDC = ModelDiscrepancy(t_obs=t_obs, hexp_mean=hexp_mean, hexp_err=hexp_err, \n",
    "                      model_height=model_height, num_model_param=num_model_param, \n",
    "                      log_priors=log_priors_thetaphi, MD = True, kernel_func = kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# Find MAP values for parameters -->\n",
    "guess = param_bounds.mean(axis=1)\n",
    "\n",
    "nlp = lambda *args: - MDC.log_posterior(*args)\n",
    "result = optimize.minimize(nlp, guess,  method = 'Powell')\n",
    "MAP = result['x']\n",
    "\n",
    "print(result)\n",
    "print(\"\\nMAP values for parameters:\", MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "min_param = param_bounds[:,0]\n",
    "max_param = param_bounds[:,1]\n",
    "lab = [r'$g$', r'$v_0$', r'$\\bar{c}$', r'$\\ell$', r'$r$', r'$s$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_burn = 100  # 2000 is a minimum\n",
    "num_steps = 100  # 2000 is a minimum\n",
    "\n",
    "samples_MD = emcee_sampling(min_param = min_param, max_param = max_param, \n",
    "                         log_posterior = MDC.log_posterior, \n",
    "                         nburn = num_burn, nsteps = num_steps, num_walker_per_dim = 8)\n",
    "\n",
    "fig = corner.corner(samples_MD, labels=lab, color='C0', quantiles=[0.16, 0.5, 0.84],\n",
    "                   show_titles=True, title_kwargs={\"fontsize\": 12}, hist_kwargs={'density': True})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sampling with pocomc for consistency check ---> \n",
    "\n",
    "# n_effective_mltpl = 500\n",
    "# n_active_mltpl = 150  # 0.3 x n_eff\n",
    "# n_steps_mltpl = 2\n",
    "# num_param = len(min_param)\n",
    "\n",
    "# n_effective = n_effective_mltpl * num_param\n",
    "# n_active = n_active_mltpl * num_param\n",
    "# n_steps = n_steps_mltpl * num_param\n",
    "# # print(n_effective, n_active, n_steps)\n",
    "\n",
    "# n_total = 20000\n",
    "\n",
    "# samples_MD_poco = pocomc_sampling(min_param, max_param, MDC.log_posterior,\n",
    "#                                   n_effective=n_effective, n_active=n_effective, n_steps=n_steps,\n",
    "#                                   n_total=n_total, n_evidence=n_total)\n",
    "\n",
    "# fig = corner.corner(samples_MD_poco, labels=lab, color='C0', quantiles=[0.16, 0.5, 0.84],\n",
    "#                    show_titles=True, title_kwargs={\"fontsize\": 12}, hist_kwargs={'density': True})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "2025-book-env",
   "language": "python",
   "name": "2025-book-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
